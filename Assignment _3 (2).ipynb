{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## MSDS 492- Analysis of Financial Markets\n",
        "## Assignment 3- GARCH Model Analysis \n",
        "## David Van Dyke"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading from Yfinance"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Download historical market data for SPY, GLD, CL=F, PSX, VLO, MPC,\n",
        "and EUR to USD exchange rate (EURUSD=X)\n",
        "and save each to a separate sheet in a single Excel file named\n",
        "'GARCH model.xlsx'.\n",
        "\n",
        "Enhancements:\n",
        "- Uses SPY, GLD, CL=F, PSX, VLO, MPC, EURUSD=X\n",
        "- Keeps yfinance's grouped (non-flattened) columns\n",
        "- Normalizes tz on the index and labels the index as 'Date'\n",
        "- Includes 'Adj Close' if available; if not, creates it by copying 'Close'\n",
        "- Writes sheets with index=True (index as first column)\n",
        "- No dividends/splits handling\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "\n",
        "# Updated tickers to include EUR->USD exchange rate\n",
        "TICKERS = [\"SPY\", \"GLD\", \"CL=F\", \"PSX\", \"VLO\", \"MPC\", \"EURUSD=X\"]\n",
        "\n",
        "OUTPUT_XLSX = \"GARCH model.xlsx\"\n",
        "\n",
        "INTERVAL = \"1d\"  # \"1d\", \"1wk\", \"1mo\"\n",
        "\n",
        "# Updated date range: 1/1/2019 → today\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE = datetime.today()\n",
        "\n",
        "# Optional: friendlier sheet names (Excel sheet names are limited to 31 chars)\n",
        "SHEET_NAME_MAP: Dict[str, str] = {\n",
        "    \"EURUSD=X\": \"EUR_USD\",\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "\n",
        "def safe_sheet_name(name: str) -> str:\n",
        "    \"\"\"\n",
        "    Excel sheet names are limited to 31 chars and cannot contain: : \\ / ? * [ ]\n",
        "    Trim and replace invalid characters to keep it safe.\n",
        "    \"\"\"\n",
        "    invalid = [\":\", \"\\\\\", \"/\", \"?\", \"*\", \"[\", \"]\"]\n",
        "    for ch in invalid:\n",
        "        name = name.replace(ch, \"-\")\n",
        "    return name[:31]\n",
        "\n",
        "\n",
        "def ensure_adj_close(df: pd.DataFrame) -> Tuple[pd.DataFrame, bool]:\n",
        "    \"\"\"\n",
        "    Ensure an 'Adj Close' column (or sub-column) exists.\n",
        "    - If present, returns df unchanged and flag=False.\n",
        "    - If absent, create it by copying 'Close' and return flag=True.\n",
        "\n",
        "    Supports:\n",
        "      • Single-level columns (e.g., 'Open', 'High', ..., 'Adj Close')\n",
        "      • MultiIndex columns (yfinance layout with group_by='column')\n",
        "    \"\"\"\n",
        "    created = False\n",
        "    cols = df.columns\n",
        "\n",
        "    # Case A: Single-level columns\n",
        "    if isinstance(cols, pd.Index):\n",
        "        if \"Adj Close\" in cols:\n",
        "            return df, created\n",
        "        if \"Close\" in cols:\n",
        "            df = df.copy()\n",
        "            df[\"Adj Close\"] = df[\"Close\"]\n",
        "            created = True\n",
        "        return df, created\n",
        "\n",
        "    # Case B: MultiIndex columns\n",
        "    if isinstance(cols, pd.MultiIndex):\n",
        "        level0 = cols.get_level_values(0)\n",
        "\n",
        "        if \"Adj Close\" in level0:\n",
        "            return df, created\n",
        "\n",
        "        if \"Close\" in level0:\n",
        "            df = df.copy()\n",
        "            sub_levels = cols[level0 == \"Close\"]\n",
        "            new_cols = [(\"Adj Close\", sub) for _, sub in sub_levels]\n",
        "            copied = df.loc[:, sub_levels].copy()\n",
        "            copied.columns = pd.MultiIndex.from_tuples(new_cols, names=df.columns.names)\n",
        "            df = pd.concat([df, copied], axis=1)\n",
        "            df = df.reindex(sorted(df.columns, key=lambda t: (t[0], str(t[1]))), axis=1)\n",
        "            created = True\n",
        "        return df, created\n",
        "\n",
        "    return df, created\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# yfinance wrapper\n",
        "# -----------------------------\n",
        "\n",
        "def download_ticker_df(\n",
        "    ticker: str,\n",
        "    start: datetime,\n",
        "    end: datetime,\n",
        "    interval: str = \"1d\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download price/volume/OHLCV data for a single ticker.\n",
        "    - Keeps yfinance's grouped, non-flattened columns (group_by='column').\n",
        "    - Ensures tz-naive datetime index for consistent Excel output.\n",
        "    - Ensures 'Adj Close' exists (creates from Close if provider doesn't supply it).\n",
        "    \"\"\"\n",
        "    df = yf.download(\n",
        "        tickers=ticker,\n",
        "        start=start,\n",
        "        end=end,\n",
        "        interval=interval,\n",
        "        auto_adjust=False,\n",
        "        group_by=\"column\",\n",
        "        threads=True,\n",
        "        progress=False,\n",
        "    )\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    idx_tz = getattr(df.index, \"tz\", None)\n",
        "    if idx_tz is not None:\n",
        "        df.index = df.index.tz_convert(\"UTC\").tz_localize(None)\n",
        "\n",
        "    df.index.name = \"Date\"\n",
        "    df, _ = ensure_adj_close(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main workflow\n",
        "# -----------------------------\n",
        "\n",
        "def main():\n",
        "    out_path = os.path.abspath(OUTPUT_XLSX)\n",
        "\n",
        "    with pd.ExcelWriter(out_path, engine=\"openpyxl\") as writer:\n",
        "        for ticker in TICKERS:\n",
        "            print(f\"Downloading {ticker} from {START_DATE.date()} to {END_DATE.date()} ...\")\n",
        "            prices_df = download_ticker_df(ticker, START_DATE, END_DATE, INTERVAL)\n",
        "\n",
        "            # Prefer friendly names when provided\n",
        "            raw_sheet_name = SHEET_NAME_MAP.get(ticker, ticker)\n",
        "            sheet_name = safe_sheet_name(raw_sheet_name)\n",
        "\n",
        "            if prices_df.empty:\n",
        "                pd.DataFrame({\n",
        "                    \"Info\": [f\"No price data returned for {ticker} in the requested window.\"]\n",
        "                }).to_excel(writer, sheet_name=sheet_name, index=True)\n",
        "            else:\n",
        "                prices_df.to_excel(writer, sheet_name=sheet_name, index=True)\n",
        "\n",
        "    print(f\"✅ Excel file saved: {out_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading SPY from 2019-01-01 to 2026-02-16 ...\nDownloading GLD from 2019-01-01 to 2026-02-16 ...\nDownloading CL=F from 2019-01-01 to 2026-02-16 ...\nDownloading PSX from 2019-01-01 to 2026-02-16 ...\nDownloading VLO from 2019-01-01 to 2026-02-16 ...\nDownloading MPC from 2019-01-01 to 2026-02-16 ...\nDownloading EURUSD=X from 2019-01-01 to 2026-02-16 ...\n✅ Excel file saved: /mnt/batch/tasks/shared/LS_root/mounts/clusters/dvandyk1/code/Users/dvandyk/NW Data Science/MSDS 492 Analysis of Financial Markets/Assignment 3/GARCH model.xlsx\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1771272756773
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read yfinance Excel Workbook Into Pandas Dataframe"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import Dict, Optional, Tuple, Union\n",
        "\n",
        "def _concat_two_levels_to_str(level0: Optional[object], level1: Optional[object], sep: str) -> str:\n",
        "    \"\"\"\n",
        "    Helper to combine two header cells into a single string, handling None/NaN.\n",
        "    Converts values to strings, strips whitespace, and drops blank parts.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    for v in (level0, level1):\n",
        "        if v is None:\n",
        "            continue\n",
        "        s = str(v).strip()\n",
        "        if s != \"\" and s.lower() != \"nan\":\n",
        "            parts.append(s)\n",
        "    return sep.join(parts) if parts else \"\"\n",
        "\n",
        "\n",
        "def _flatten_columns(cols: Union[pd.Index, pd.MultiIndex], sep: str = \"|\") -> pd.Index:\n",
        "    \"\"\"\n",
        "    Flatten columns for easier downstream use.\n",
        "    - If MultiIndex: join (level0, level1) as \"level0|level1\"\n",
        "    - If Index: return as-is\n",
        "    \"\"\"\n",
        "    if isinstance(cols, pd.MultiIndex):\n",
        "        flat = []\n",
        "        for a, b in cols.to_list():\n",
        "            flat.append(_concat_two_levels_to_str(a, b, sep=sep))\n",
        "        return pd.Index(flat)\n",
        "    return cols\n",
        "\n",
        "\n",
        "def read_market_prices_xlsx(\n",
        "    path: str,\n",
        "    parse_dates: bool = True,\n",
        "    sep: str = \"|\",\n",
        "    numeric_cleanup: bool = True\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Reads the price sheets created by the download script (GARCH model.xlsx).\n",
        "    Works with sheets written using DataFrame.to_excel(index=True) where:\n",
        "      - The first column is the Date index\n",
        "      - Columns may be single-level or MultiIndex (yfinance grouped layout)\n",
        "\n",
        "    Returns a dict of DataFrames keyed by sheet name.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : str\n",
        "        Path to the Excel file.\n",
        "    parse_dates : bool\n",
        "        Convert first column to datetime index if True.\n",
        "    sep : str\n",
        "        Separator for flattening MultiIndex headers.\n",
        "    numeric_cleanup : bool\n",
        "        Strip formatting characters before numeric conversion.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, pd.DataFrame]\n",
        "        Mapping sheet_name -> cleaned DataFrame.\n",
        "    \"\"\"\n",
        "    xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
        "    out: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "    for sheet_name in xls.sheet_names:\n",
        "        # Try reading as MultiIndex header first (common with yfinance \"group_by='column'\")\n",
        "        df = None\n",
        "        try:\n",
        "            df = pd.read_excel(\n",
        "                path,\n",
        "                sheet_name=sheet_name,\n",
        "                header=[0, 1],\n",
        "                index_col=0,\n",
        "                engine=\"openpyxl\"\n",
        "            )\n",
        "            # If the second header row is effectively empty/NaN, this can produce odd columns.\n",
        "            # We'll detect that and re-read as single header.\n",
        "            if isinstance(df.columns, pd.MultiIndex):\n",
        "                # If all second-level entries are NaN/blank, treat as single-level\n",
        "                lvl1 = df.columns.get_level_values(1)\n",
        "                if all((str(v).strip().lower() in (\"nan\", \"\") for v in lvl1)):\n",
        "                    raise ValueError(\"Second header level empty; re-reading as single header.\")\n",
        "        except Exception:\n",
        "            # Fallback: single header\n",
        "            df = pd.read_excel(\n",
        "                path,\n",
        "                sheet_name=sheet_name,\n",
        "                header=0,\n",
        "                index_col=0,\n",
        "                engine=\"openpyxl\"\n",
        "            )\n",
        "\n",
        "        # If empty or just an info message sheet, keep but cleaned\n",
        "        if df is None or df.empty:\n",
        "            out[sheet_name] = pd.DataFrame()\n",
        "            continue\n",
        "\n",
        "        # Normalize index to datetime if requested\n",
        "        if parse_dates:\n",
        "            df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "            df = df[~df.index.isna()]\n",
        "\n",
        "        df.index.name = \"Date\"\n",
        "\n",
        "        # Drop all-NaN columns\n",
        "        df = df.dropna(axis=1, how=\"all\")\n",
        "\n",
        "        # Flatten columns (MultiIndex -> strings)\n",
        "        df.columns = _flatten_columns(df.columns, sep=sep)\n",
        "\n",
        "        # Ensure unique column names\n",
        "        if len(df.columns) != len(set(df.columns)):\n",
        "            seen = {}\n",
        "            newcols = []\n",
        "            for c in df.columns:\n",
        "                if c not in seen:\n",
        "                    seen[c] = 1\n",
        "                    newcols.append(c)\n",
        "                else:\n",
        "                    seen[c] += 1\n",
        "                    newcols.append(f\"{c}{sep}{seen[c]}\")\n",
        "            df.columns = newcols\n",
        "\n",
        "        # Clean numeric formatting\n",
        "        if numeric_cleanup:\n",
        "            # Work column-wise for object/string columns\n",
        "            for c in df.columns:\n",
        "                if df[c].dtype == \"object\":\n",
        "                    df[c] = df[c].astype(str).str.strip()\n",
        "                    df[c] = df[c].replace({\"\": None, \"nan\": None, \"NaN\": None})\n",
        "                    df[c] = df[c].str.replace(\",\", \"\", regex=False)\n",
        "                    df[c] = df[c].str.replace(\"%\", \"\", regex=False)\n",
        "\n",
        "        # Convert everything to numeric where possible\n",
        "        df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "        out[sheet_name] = df\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Example usage\n",
        "# ------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Read the Excel produced by your GARCH data pull\n",
        "    xlsx_path = \"GARCH model.xlsx\"\n",
        "\n",
        "    # Load all sheets\n",
        "    frames = read_market_prices_xlsx(\n",
        "        xlsx_path,\n",
        "        sep=\"|\",\n",
        "        numeric_cleanup=True,\n",
        "        parse_dates=True\n",
        "    )\n",
        "\n",
        "    # Create a DataFrame per ticker (matching sheet names in the Excel file)\n",
        "    spy_df = frames.get(\"SPY\")\n",
        "    gld_df = frames.get(\"GLD\")\n",
        "    clf_df = frames.get(\"CL=F\")\n",
        "    psx_df = frames.get(\"PSX\")\n",
        "    vlo_df = frames.get(\"VLO\")\n",
        "    mpc_df = frames.get(\"MPC\")\n",
        "    eurusd_df = frames.get(\"EUR_USD\")  "
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1771455067494
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA on GARCH Model Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "EDA for GARCH model inputs using the 'GARCH model.xlsx' file produced by the\n",
        "download script (with yfinance grouped columns) and the updated reader that\n",
        "returns cleaned DataFrames per sheet.\n",
        "\n",
        "Assets (example):\n",
        "- SPY, GLD, CL=F, PSX, VLO, MPC\n",
        "- FX: EURUSD=X saved as sheet \"EUR_USD\"\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from typing import Dict, List, Optional, Union\n",
        "\n",
        "# ----------------------------\n",
        "# Load your data (updated inline reader for the NEW Excel layout)\n",
        "# ----------------------------\n",
        "def _concat_two_levels_to_str(level0: Optional[object], level1: Optional[object], sep: str) -> str:\n",
        "    \"\"\"\n",
        "    Helper to combine two header cells into a single string, handling None/NaN.\n",
        "    Converts values to strings, strips whitespace, and drops blank parts.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    for v in (level0, level1):\n",
        "        if v is None:\n",
        "            continue\n",
        "        s = str(v).strip()\n",
        "        if s != \"\" and s.lower() != \"nan\":\n",
        "            parts.append(s)\n",
        "    return sep.join(parts) if parts else \"\"\n",
        "\n",
        "\n",
        "def _flatten_columns(cols: Union[pd.Index, pd.MultiIndex], sep: str = \"|\") -> pd.Index:\n",
        "    \"\"\"\n",
        "    Flatten columns for easier downstream use.\n",
        "    - If MultiIndex: join (level0, level1) as \"level0|level1\"\n",
        "    - If Index: return as-is\n",
        "    \"\"\"\n",
        "    if isinstance(cols, pd.MultiIndex):\n",
        "        flat = []\n",
        "        for a, b in cols.to_list():\n",
        "            flat.append(_concat_two_levels_to_str(a, b, sep=sep))\n",
        "        return pd.Index(flat)\n",
        "    return cols\n",
        "\n",
        "\n",
        "def read_market_prices_xlsx(\n",
        "    path: str,\n",
        "    parse_dates: bool = True,\n",
        "    sep: str = \"|\",\n",
        "    numeric_cleanup: bool = True\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Reads the price sheets created by the download script (GARCH model.xlsx).\n",
        "    Works with sheets written using DataFrame.to_excel(index=True) where:\n",
        "      - The first column is the Date index\n",
        "      - Columns may be single-level or MultiIndex (yfinance grouped layout)\n",
        "\n",
        "    Returns a dict of DataFrames keyed by sheet name.\n",
        "    \"\"\"\n",
        "    xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
        "    out: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "    for sheet_name in xls.sheet_names:\n",
        "        # Try MultiIndex header first (typical yfinance \"group_by='column'\")\n",
        "        try:\n",
        "            df = pd.read_excel(\n",
        "                path,\n",
        "                sheet_name=sheet_name,\n",
        "                header=[0, 1],\n",
        "                index_col=0,\n",
        "                engine=\"openpyxl\"\n",
        "            )\n",
        "            # If second header level is effectively empty, re-read as single header\n",
        "            if isinstance(df.columns, pd.MultiIndex):\n",
        "                lvl1 = df.columns.get_level_values(1)\n",
        "                if all((str(v).strip().lower() in (\"nan\", \"\") for v in lvl1)):\n",
        "                    raise ValueError(\"Second header level empty; re-reading as single header.\")\n",
        "        except Exception:\n",
        "            df = pd.read_excel(\n",
        "                path,\n",
        "                sheet_name=sheet_name,\n",
        "                header=0,\n",
        "                index_col=0,\n",
        "                engine=\"openpyxl\"\n",
        "            )\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            out[sheet_name] = pd.DataFrame()\n",
        "            continue\n",
        "\n",
        "        # Parse Date index\n",
        "        if parse_dates:\n",
        "            df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
        "            df = df[~df.index.isna()]\n",
        "        df.index.name = \"Date\"\n",
        "\n",
        "        # Drop all-NaN columns\n",
        "        df = df.dropna(axis=1, how=\"all\")\n",
        "\n",
        "        # Flatten columns (MultiIndex -> strings)\n",
        "        df.columns = _flatten_columns(df.columns, sep=sep)\n",
        "\n",
        "        # Ensure unique column names\n",
        "        if len(df.columns) != len(set(df.columns)):\n",
        "            seen = {}\n",
        "            newcols = []\n",
        "            for c in df.columns:\n",
        "                if c not in seen:\n",
        "                    seen[c] = 1\n",
        "                    newcols.append(c)\n",
        "                else:\n",
        "                    seen[c] += 1\n",
        "                    newcols.append(f\"{c}{sep}{seen[c]}\")\n",
        "            df.columns = newcols\n",
        "\n",
        "        # Clean numeric formatting\n",
        "        if numeric_cleanup:\n",
        "            for c in df.columns:\n",
        "                if df[c].dtype == \"object\":\n",
        "                    df[c] = df[c].astype(str).str.strip()\n",
        "                    df[c] = df[c].replace({\"\": None, \"nan\": None, \"NaN\": None})\n",
        "                    df[c] = df[c].str.replace(\",\", \"\", regex=False)\n",
        "                    df[c] = df[c].str.replace(\"%\", \"\", regex=False)\n",
        "\n",
        "        # Convert to numeric\n",
        "        df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "        out[sheet_name] = df\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Plot styling\n",
        "# ----------------------------\n",
        "plt.rcParams[\"figure.figsize\"] = (11, 7)\n",
        "plt.rcParams[\"axes.grid\"] = True\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers (updated/complete)\n",
        "# ----------------------------\n",
        "def _select_numeric_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "\n",
        "def _infer_close_col(df: pd.DataFrame) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Works with flattened columns like:\n",
        "      - 'Adj Close|SPY', 'Close|SPY', etc.\n",
        "    Also works if columns are single-level like 'Adj Close' or 'Close'.\n",
        "    Preference order:\n",
        "      1) Adj Close\n",
        "      2) Close\n",
        "      3) anything containing 'close'\n",
        "    \"\"\"\n",
        "    cols_lower = {str(c).lower(): c for c in df.columns}\n",
        "\n",
        "    # Prefer exact / contains patterns\n",
        "    adj = next((cols_lower[k] for k in cols_lower if \"adj close\" in k), None)\n",
        "    close_exact = next((cols_lower[k] for k in cols_lower if k == \"close\"), None)\n",
        "    close_any = next((cols_lower[k] for k in cols_lower if \"close\" in k), None)\n",
        "\n",
        "    return next((c for c in (adj, close_exact, close_any) if c is not None), None)\n",
        "\n",
        "\n",
        "def _daily_returns(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Log returns: ln(P_t) - ln(P_{t-1})\n",
        "    \"\"\"\n",
        "    s = pd.to_numeric(series, errors=\"coerce\").astype(float)\n",
        "    s = s.replace([0, np.inf, -np.inf], np.nan).dropna()\n",
        "    return np.log(s).diff()\n",
        "\n",
        "\n",
        "def _safe_title(name: str) -> str:\n",
        "    return str(name).strip() if name else \"Unknown\"\n",
        "\n",
        "\n",
        "def _inject_index_as_column(df: pd.DataFrame, name: str = \"Variable\") -> pd.DataFrame:\n",
        "    idx_name = df.index.name if df.index.name not in [None, \"\"] else name\n",
        "    out = df.reset_index()\n",
        "    if out.columns[0] != idx_name:\n",
        "        out = out.rename(columns={out.columns[0]: idx_name})\n",
        "    return out\n",
        "\n",
        "\n",
        "def _table_figure_from_dataframe(df: pd.DataFrame, title: str, font_size: int = 9) -> plt.Figure:\n",
        "    fig_height = min(7, 1 + 0.35 * (len(df) + 2))\n",
        "    fig, ax = plt.subplots(figsize=(11, fig_height))\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(title, fontsize=14, pad=12, loc=\"left\")\n",
        "    tbl = ax.table(\n",
        "        cellText=df.round(4).values.tolist(),\n",
        "        colLabels=list(df.columns),\n",
        "        loc=\"center\",\n",
        "        cellLoc=\"right\",\n",
        "    )\n",
        "    tbl.auto_set_font_size(False)\n",
        "    tbl.set_fontsize(font_size)\n",
        "    tbl.scale(1, 1.25)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def _corr_heatmap(df: pd.DataFrame, title: str) -> plt.Figure:\n",
        "    num = _select_numeric_cols(df)\n",
        "    if num.empty:\n",
        "        msg = pd.DataFrame({\"Message\": [\"No numeric columns found.\"]})\n",
        "        msg = _inject_index_as_column(msg, \"Info\")\n",
        "        return _table_figure_from_dataframe(msg, title)\n",
        "\n",
        "    corr = num.corr()\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(corr, cmap=\"coolwarm\", center=0, ax=ax)\n",
        "    ax.set_title(title)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def _rolling_volatility(series: pd.Series, window: int = 30) -> pd.Series:\n",
        "    rets = _daily_returns(series).dropna()\n",
        "    return rets.rolling(window).std() * math.sqrt(252)\n",
        "\n",
        "\n",
        "def _schema_and_missing_table(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Summarize column schema + missingness.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    n = len(df)\n",
        "    for c in df.columns:\n",
        "        s = df[c]\n",
        "        miss = int(s.isna().sum())\n",
        "        nonmiss = int(s.notna().sum())\n",
        "        rows.append({\n",
        "            \"dtype\": str(s.dtype),\n",
        "            \"count\": nonmiss,\n",
        "            \"missing\": miss,\n",
        "            \"missing_pct\": (miss / n * 100.0) if n else np.nan,\n",
        "        })\n",
        "    out = pd.DataFrame(rows, index=df.columns)\n",
        "    out.index.name = \"Variable\"\n",
        "    return out.sort_values([\"missing\", \"dtype\"], ascending=[False, True])\n",
        "\n",
        "\n",
        "def _numeric_quality_and_ranges(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    For numeric columns: counts, missing, min/max, and basic data quality flags.\n",
        "    \"\"\"\n",
        "    num = _select_numeric_cols(df)\n",
        "    if num.empty:\n",
        "        out = pd.DataFrame({\"Message\": [\"No numeric columns found.\"]}, index=[\"Info\"])\n",
        "        out.index.name = \"Variable\"\n",
        "        return out\n",
        "\n",
        "    n = len(num)\n",
        "    rows = []\n",
        "    for c in num.columns:\n",
        "        s = num[c].astype(float)\n",
        "        miss = int(s.isna().sum())\n",
        "        nonmiss = int(s.notna().sum())\n",
        "        zeros = int((s == 0).sum(skipna=True))\n",
        "        neg = int((s < 0).sum(skipna=True))\n",
        "        infs = int(np.isinf(s.to_numpy(dtype=float, na_value=np.nan)).sum())  # mostly 0 after coercion\n",
        "        rows.append({\n",
        "            \"count\": nonmiss,\n",
        "            \"missing\": miss,\n",
        "            \"missing_pct\": (miss / n * 100.0) if n else np.nan,\n",
        "            \"zeros\": zeros,\n",
        "            \"negatives\": neg,\n",
        "            \"infs\": infs,\n",
        "            \"min\": float(np.nanmin(s.to_numpy())) if nonmiss else np.nan,\n",
        "            \"max\": float(np.nanmax(s.to_numpy())) if nonmiss else np.nan,\n",
        "            \"mean\": float(np.nanmean(s.to_numpy())) if nonmiss else np.nan,\n",
        "            \"std\": float(np.nanstd(s.to_numpy(), ddof=1)) if nonmiss > 1 else np.nan,\n",
        "        })\n",
        "\n",
        "    out = pd.DataFrame(rows, index=num.columns)\n",
        "    out.index.name = \"Variable\"\n",
        "    return out.sort_values([\"missing\", \"std\"], ascending=[False, False])\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Single-ticker EDA\n",
        "# ----------------------------\n",
        "def eda_single_ticker(name: str, df: pd.DataFrame) -> List[plt.Figure]:\n",
        "    figs: List[plt.Figure] = []\n",
        "    ticker = _safe_title(name)\n",
        "    df = df.copy().sort_index()\n",
        "\n",
        "    # Schema\n",
        "    tab = _schema_and_missing_table(df)\n",
        "    tab = _inject_index_as_column(tab, \"Variable\")\n",
        "    figs.append(_table_figure_from_dataframe(tab, f\"{ticker} • Schema & Missingness\"))\n",
        "\n",
        "    # Numeric quality\n",
        "    tab2 = _numeric_quality_and_ranges(df)\n",
        "    tab2 = _inject_index_as_column(tab2, \"Variable\")\n",
        "    figs.append(_table_figure_from_dataframe(tab2, f\"{ticker} • Numeric Quality & Ranges\"))\n",
        "\n",
        "    # Describe\n",
        "    num = _select_numeric_cols(df)\n",
        "    if not num.empty:\n",
        "        desc = num.describe().T\n",
        "        desc = _inject_index_as_column(desc, \"Variable\")\n",
        "        figs.append(_table_figure_from_dataframe(desc, f\"{ticker} • Describe\"))\n",
        "    else:\n",
        "        msg = pd.DataFrame({\"Message\": [\"No numeric columns.\"]})\n",
        "        msg = _inject_index_as_column(msg)\n",
        "        figs.append(_table_figure_from_dataframe(msg, f\"{ticker} • Describe\"))\n",
        "\n",
        "    # Correlation\n",
        "    figs.append(_corr_heatmap(df, f\"{ticker} • Correlation\"))\n",
        "\n",
        "    # Prices + volatility + returns\n",
        "    close_col = _infer_close_col(df)\n",
        "    if close_col:\n",
        "        price = pd.to_numeric(df[close_col], errors=\"coerce\").astype(float)\n",
        "\n",
        "        # Price plot\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.plot(price.index, price.values)\n",
        "        ax.set_title(f\"{ticker} • Price ({close_col})\")\n",
        "        figs.append(fig)\n",
        "\n",
        "        # Rolling vol plot\n",
        "        fig, ax = plt.subplots()\n",
        "        vol = _rolling_volatility(price)\n",
        "        ax.plot(vol.index, vol.values)\n",
        "        ax.set_title(f\"{ticker} • 30D Volatility (ann.)\")\n",
        "        figs.append(fig)\n",
        "\n",
        "        # Returns histogram\n",
        "        fig, ax = plt.subplots()\n",
        "        rets = _daily_returns(price).dropna()\n",
        "        if not rets.empty:\n",
        "            sns.histplot(rets, bins=50, kde=True, ax=ax)\n",
        "            ax.set_title(f\"{ticker} • Daily Log Returns\")\n",
        "        else:\n",
        "            ax.text(0.1, 0.5, \"Returns series is empty after cleaning.\", fontsize=12)\n",
        "            ax.set_title(f\"{ticker} • Daily Log Returns\")\n",
        "        figs.append(fig)\n",
        "\n",
        "    return figs\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Cross-ticker EDA\n",
        "# ----------------------------\n",
        "def eda_cross_ticker(frames: Dict[str, pd.DataFrame]) -> List[plt.Figure]:\n",
        "    figs: List[plt.Figure] = []\n",
        "\n",
        "    price_map = {}\n",
        "    for name, df in frames.items():\n",
        "        col = _infer_close_col(df)\n",
        "        if col:\n",
        "            price_map[name] = pd.to_numeric(df[col], errors=\"coerce\").astype(float)\n",
        "\n",
        "    if price_map:\n",
        "        prices = pd.DataFrame(price_map).dropna(how=\"all\")\n",
        "\n",
        "        # Overlay prices\n",
        "        fig, ax = plt.subplots()\n",
        "        for name in prices.columns:\n",
        "            ax.plot(prices.index, prices[name], label=name)\n",
        "        ax.set_title(\"Cross-Ticker Prices (Close/Adj Close)\")\n",
        "        ax.legend()\n",
        "        figs.append(fig)\n",
        "\n",
        "        # Return correlation\n",
        "        rets = prices.apply(lambda s: _daily_returns(s)).dropna(how=\"all\")\n",
        "        if not rets.empty:\n",
        "            corr = rets.corr()\n",
        "            fig, ax = plt.subplots()\n",
        "            sns.heatmap(corr, annot=True, cmap=\"viridis\", ax=ax)\n",
        "            ax.set_title(\"Cross-Ticker Return Correlation (log returns)\")\n",
        "            figs.append(fig)\n",
        "\n",
        "    return figs\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Report Orchestration\n",
        "# ----------------------------\n",
        "def build_garch_report(\n",
        "    frames: Dict[str, pd.DataFrame],\n",
        "    pdf_title: str = \"EDA GARCH model report.pdf\",\n",
        "    show_plots: bool = False,\n",
        "):\n",
        "    figs: List[plt.Figure] = []\n",
        "    ordered = sorted(frames.items(), key=lambda kv: kv[0])\n",
        "\n",
        "    for name, df in ordered:\n",
        "        figs.extend(eda_single_ticker(name, df))\n",
        "\n",
        "    figs.extend(eda_cross_ticker(frames))\n",
        "\n",
        "    with PdfPages(pdf_title) as pdf:\n",
        "        cover, ax = plt.subplots(figsize=(11, 8))\n",
        "        ax.axis(\"off\")\n",
        "        ax.text(0, 0.95, \"EDA GARCH Model Report\", fontsize=24, weight=\"bold\")\n",
        "        ax.text(0, 0.88, f\"Sheets: {', '.join(frames.keys())}\", fontsize=12)\n",
        "        ax.text(0, 0.82, \"Generated automatically.\", fontsize=12)\n",
        "        pdf.savefig(cover)\n",
        "        plt.close(cover)\n",
        "\n",
        "        for fig in figs:\n",
        "            pdf.savefig(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "    print(f\"✅ PDF saved: {pdf_title}\")\n",
        "\n",
        "    if show_plots:\n",
        "        for fig in figs:\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Example usage\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    xlsx_path = \"GARCH model.xlsx\"\n",
        "\n",
        "    # Read using the NEW reader that matches the new workbook structure\n",
        "    frames = read_market_prices_xlsx(\n",
        "        xlsx_path,\n",
        "        sep=\"|\",\n",
        "        numeric_cleanup=True,\n",
        "        parse_dates=True\n",
        "    )\n",
        "\n",
        "    # Target assets: original set + FX sheet\n",
        "    # FX sheet name is \"EUR_USD\" if you used SHEET_NAME_MAP in the download script,\n",
        "    # otherwise it may be \"EURUSD=X\". We accept either.\n",
        "    target = {\"SPY\", \"GLD\", \"CL=F\", \"PSX\", \"VLO\", \"MPC\", \"EUR_USD\", \"EURUSD=X\"}\n",
        "    frames = {k: v for k, v in frames.items() if k in target and v is not None and not v.empty}\n",
        "\n",
        "    if not frames:\n",
        "        raise RuntimeError(\"No valid sheets found in GARCH model.xlsx for the target set.\")\n",
        "\n",
        "    build_garch_report(frames)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/tmp/ipykernel_11276/2035653122.py:342: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n  fig, ax = plt.subplots()\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "✅ PDF saved: EDA GARCH model report.pdf\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1771455124927
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Engineered Features"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, Optional\n",
        "\n",
        "# ================================================================\n",
        "# Helpers\n",
        "# ================================================================\n",
        "def _find_col(df: pd.DataFrame, keyword: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Find the first column whose name contains `keyword` (case-insensitive).\n",
        "    \"\"\"\n",
        "    keyword = keyword.lower()\n",
        "    for c in df.columns:\n",
        "        if keyword in str(c).lower():\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "\n",
        "def _safe_numeric(s: pd.Series) -> pd.Series:\n",
        "    return pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Feature Engineering for GARCH-ready series\n",
        "# ================================================================\n",
        "def engineer_features_for_ticker(\n",
        "    df: pd.DataFrame,\n",
        "    ticker: str,\n",
        "    ewma_lambda: float = 0.94\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds engineered features for a given asset DataFrame.\n",
        "\n",
        "    Compatible with the NEW flattened yfinance layout:\n",
        "        e.g. 'Adj Close|SPY', 'Close|SPY', etc.\n",
        "\n",
        "    Core GARCH-oriented features:\n",
        "        - log_ret, ret_sq, ret_abs\n",
        "        - lagged returns (1–3)\n",
        "        - lagged squared returns (1–3)\n",
        "        - EWMA variance (RiskMetrics)\n",
        "        - rolling variance (20)\n",
        "\n",
        "    Additional diagnostics / exploratory:\n",
        "        - HL / OC spreads (if available)\n",
        "        - SMA / EMA (5/10/20)\n",
        "        - lagged prices (1–3)\n",
        "        - autocorr snapshots\n",
        "        - calendar seasonality\n",
        "    \"\"\"\n",
        "\n",
        "    out = df.copy().sort_index()\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Seasonal / calendar features\n",
        "    # ------------------------------------------------\n",
        "    out[\"month\"]          = out.index.month\n",
        "    out[\"day_of_week\"]    = out.index.dayofweek\n",
        "    out[\"quarter\"]        = out.index.quarter\n",
        "    out[\"year\"]           = out.index.year\n",
        "    out[\"is_month_end\"]   = out.index.is_month_end.astype(int)\n",
        "    out[\"is_month_start\"] = out.index.is_month_start.astype(int)\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Infer price-related columns (robust)\n",
        "    # ------------------------------------------------\n",
        "    adj_close_col = _find_col(out, \"adj close\")\n",
        "    close_col     = _find_col(out, \"close\")\n",
        "    high_col      = _find_col(out, \"high\")\n",
        "    low_col       = _find_col(out, \"low\")\n",
        "    open_col      = _find_col(out, \"open\")\n",
        "\n",
        "    # Prefer Adj Close when available\n",
        "    price_col = adj_close_col if adj_close_col is not None else close_col\n",
        "    if price_col is None:\n",
        "        raise ValueError(f\"No Close or Adj Close column found for {ticker}\")\n",
        "\n",
        "    price = _safe_numeric(out[price_col])\n",
        "    high  = _safe_numeric(out[high_col])  if high_col  else None\n",
        "    low   = _safe_numeric(out[low_col])   if low_col   else None\n",
        "    open_ = _safe_numeric(out[open_col])  if open_col  else None\n",
        "    close = _safe_numeric(out[close_col]) if close_col else None\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Spread features (only if data exists)\n",
        "    # ------------------------------------------------\n",
        "    out[f\"HL_spread|{ticker}\"] = (high - low) if (high is not None and low is not None) else np.nan\n",
        "    out[f\"OC_spread|{ticker}\"] = (open_ - close) if (open_ is not None and close is not None) else np.nan\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # SMA / EMA (exploratory)\n",
        "    # ------------------------------------------------\n",
        "    for w in (5, 10, 20):\n",
        "        out[f\"SMA_{w}|{ticker}\"] = price.rolling(w, min_periods=1).mean()\n",
        "        out[f\"EMA_{w}|{ticker}\"] = price.ewm(span=w, adjust=False, min_periods=1).mean()\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Lagged prices (exploratory)\n",
        "    # ------------------------------------------------\n",
        "    for k in (1, 2, 3):\n",
        "        out[f\"lag_price_{k}|{ticker}\"] = price.shift(k)\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Price autocorrelation snapshots (scalar, repeated)\n",
        "    # ------------------------------------------------\n",
        "    for k in (1, 2, 3):\n",
        "        out[f\"autocorr_price_{k}|{ticker}\"] = price.corr(price.shift(k))\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Log returns (CORE for GARCH)\n",
        "    # ------------------------------------------------\n",
        "    out[f\"log_ret|{ticker}\"] = np.log(price / price.shift(1))\n",
        "    ret = out[f\"log_ret|{ticker}\"]\n",
        "\n",
        "    out[f\"ret_sq|{ticker}\"]  = ret ** 2\n",
        "    out[f\"ret_abs|{ticker}\"] = ret.abs()\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Lagged returns (ARCH diagnostics)\n",
        "    # ------------------------------------------------\n",
        "    for k in (1, 2, 3):\n",
        "        out[f\"lag_ret_{k}|{ticker}\"]    = ret.shift(k)\n",
        "        out[f\"lag_ret_sq_{k}|{ticker}\"] = (ret ** 2).shift(k)\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Variance proxies\n",
        "    # ------------------------------------------------\n",
        "    # RiskMetrics EWMA variance\n",
        "    out[f\"ewma_var|{ticker}\"] = (\n",
        "        ret.pow(2)\n",
        "           .ewm(alpha=(1 - ewma_lambda), adjust=False)\n",
        "           .mean()\n",
        "    )\n",
        "\n",
        "    # Rolling variance\n",
        "    out[f\"roll_var_20|{ticker}\"] = ret.rolling(20, min_periods=5).var()\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Return autocorrelation snapshots\n",
        "    # ------------------------------------------------\n",
        "    for k in (1, 2, 3):\n",
        "        out[f\"autocorr_ret_{k}|{ticker}\"] = ret.corr(ret.shift(k))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Apply feature engineering across all assets\n",
        "# ================================================================\n",
        "def engineer_features(frames: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Applies feature engineering to each asset DataFrame.\n",
        "\n",
        "    Assumes:\n",
        "        frames keys = sheet names\n",
        "        frames values = cleaned DataFrames from the NEW reader\n",
        "    \"\"\"\n",
        "    engineered: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "    for name, df in frames.items():\n",
        "        if df is None or df.empty:\n",
        "            continue\n",
        "        engineered[name] = engineer_features_for_ticker(df, name)\n",
        "\n",
        "    return engineered\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Example usage\n",
        "# ================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Assumes you already ran:\n",
        "    # frames = read_market_prices_xlsx(\"GARCH model.xlsx\", parse_dates=True)\n",
        "\n",
        "    engineered = engineer_features(frames)\n",
        "\n",
        "    spy_df    = engineered.get(\"SPY\")\n",
        "    gld_df    = engineered.get(\"GLD\")\n",
        "    clf_df    = engineered.get(\"CL=F\")\n",
        "    psx_df    = engineered.get(\"PSX\")\n",
        "    vlo_df    = engineered.get(\"VLO\")\n",
        "    mpc_df    = engineered.get(\"MPC\")\n",
        "    eurusd_df = engineered.get(\"EUR_USD\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1771455167703
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA on Engineered Features"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Engineered Feature EDA Report for GARCH Model Inputs:\n",
        "SPY, GLD, CL=F, PSX, VLO, MPC, EUR_USD\n",
        "\n",
        "Runs directly on already-loaded DataFrames:\n",
        "\n",
        "    spy_df, gld_df, clf_df, psx_df, vlo_df, mpc_df, EUR_USD\n",
        "\n",
        "Does NOT read Excel files.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "# --------------------------------------\n",
        "# Plot Styling\n",
        "# --------------------------------------\n",
        "plt.rcParams[\"figure.figsize\"] = (11, 7)\n",
        "plt.rcParams[\"axes.grid\"] = True\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# Helpers\n",
        "# --------------------------------------\n",
        "def _safe_title(name: str) -> str:\n",
        "    return str(name).strip() if name else \"Unknown\"\n",
        "\n",
        "def _select_numeric_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "def _infer_close_col(df: pd.DataFrame) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Identify a 'Close-like' column such as:\n",
        "      Adj Close|SPY\n",
        "      Close|SPY\n",
        "      close or adj close anywhere in the name\n",
        "    \"\"\"\n",
        "    lower_map = {str(c).lower(): c for c in df.columns}\n",
        "\n",
        "    # Priority 1: Adj Close\n",
        "    for key, orig in lower_map.items():\n",
        "        if \"adj close\" in key:\n",
        "            return orig\n",
        "\n",
        "    # Priority 2: exact \"close\"\n",
        "    if \"close\" in lower_map:\n",
        "        return lower_map[\"close\"]\n",
        "\n",
        "    # Priority 3: any containing \"close\"\n",
        "    for key, orig in lower_map.items():\n",
        "        if \"close\" in key:\n",
        "            return orig\n",
        "\n",
        "    return None\n",
        "\n",
        "def _daily_returns(series: pd.Series) -> pd.Series:\n",
        "    return np.log(series).diff()\n",
        "\n",
        "def _inject_index_as_column(df: pd.DataFrame, name: str = \"Variable\") -> pd.DataFrame:\n",
        "    idx_name = df.index.name if df.index.name not in [None, \"\"] else name\n",
        "    out = df.reset_index()\n",
        "    if out.columns[0] != idx_name:\n",
        "        out = out.rename(columns={out.columns[0]: idx_name})\n",
        "    return out\n",
        "\n",
        "def _table_figure_from_dataframe(df: pd.DataFrame, title: str, font_size: int = 9) -> plt.Figure:\n",
        "    fig_height = min(7, 1 + 0.35 * (len(df) + 2))\n",
        "    fig, ax = plt.subplots(figsize=(11, fig_height))\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(title, fontsize=14, loc=\"left\")\n",
        "\n",
        "    tbl = ax.table(\n",
        "        cellText=df.round(4).values.tolist(),\n",
        "        colLabels=list(df.columns),\n",
        "        loc=\"center\",\n",
        "        cellLoc=\"right\"\n",
        "    )\n",
        "    tbl.auto_set_font_size(False)\n",
        "    tbl.set_fontsize(font_size)\n",
        "    tbl.scale(1, 1.25)\n",
        "    return fig\n",
        "\n",
        "def _corr_heatmap(df: pd.DataFrame, title: str) -> plt.Figure:\n",
        "    num = _select_numeric_cols(df)\n",
        "    if num.empty:\n",
        "        msg = pd.DataFrame({\"Message\": [\"No numeric columns found.\"]})\n",
        "        msg = _inject_index_as_column(msg)\n",
        "        return _table_figure_from_dataframe(msg, title)\n",
        "\n",
        "    corr = num.corr()\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(corr, cmap=\"coolwarm\", center=0, ax=ax)\n",
        "    ax.set_title(title)\n",
        "    return fig\n",
        "\n",
        "def _rolling_volatility(series: pd.Series, window: int = 30) -> pd.Series:\n",
        "    rets = np.log(series).diff().dropna()\n",
        "    return rets.rolling(window).std() * math.sqrt(252)\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# Numeric EDA Tables\n",
        "# --------------------------------------\n",
        "def _schema_and_missing_table(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    n = len(df)\n",
        "\n",
        "    for col in df.columns:\n",
        "        s = df[col]\n",
        "        non_null = s.notna().sum()\n",
        "        missing = s.isna().sum()\n",
        "\n",
        "        try:\n",
        "            unique = s.nunique(dropna=True)\n",
        "        except:\n",
        "            unique = np.nan\n",
        "\n",
        "        sample = s.dropna().iloc[0] if non_null > 0 else None\n",
        "\n",
        "        rows.append({\n",
        "            \"Column\": col,\n",
        "            \"dtype\": str(s.dtype),\n",
        "            \"non_null\": non_null,\n",
        "            \"missing\": missing,\n",
        "            \"missing_%\": round(missing / n * 100, 2),\n",
        "            \"unique\": unique,\n",
        "            \"sample\": sample\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows).set_index(\"Column\")\n",
        "\n",
        "def _numeric_quality_and_ranges(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    num = _select_numeric_cols(df)\n",
        "    if num.empty:\n",
        "        return pd.DataFrame({\"Message\": [\"No numeric columns found.\"]})\n",
        "\n",
        "    rows = []\n",
        "    num = num.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    for col in num.columns:\n",
        "        s = num[col]\n",
        "        non_null = s.notna().sum()\n",
        "        missing = s.isna().sum()\n",
        "\n",
        "        zeros = (s == 0).sum() if non_null > 0 else 0\n",
        "        negatives = (s < 0).sum() if non_null > 0 else 0\n",
        "\n",
        "        stats = {\n",
        "            \"min\": s.min(),\n",
        "            \"p01\": s.quantile(0.01),\n",
        "            \"q1\": s.quantile(0.25),\n",
        "            \"median\": s.quantile(0.50),\n",
        "            \"mean\": s.mean(),\n",
        "            \"q3\": s.quantile(0.75),\n",
        "            \"p99\": s.quantile(0.99),\n",
        "            \"max\": s.max(),\n",
        "            \"std\": s.std()\n",
        "        }\n",
        "\n",
        "        rows.append({\n",
        "            \"Column\": col,\n",
        "            \"non_null\": non_null,\n",
        "            \"missing\": missing,\n",
        "            \"missing_%\": round(missing / len(num) * 100, 2),\n",
        "            \"zeros\": zeros,\n",
        "            \"negatives\": negatives,\n",
        "            **stats\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows).set_index(\"Column\")\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# Per-Ticker EDA\n",
        "# --------------------------------------\n",
        "def eda_single_ticker(name: str, df: pd.DataFrame) -> List[plt.Figure]:\n",
        "    figs = []\n",
        "    df = df.copy().sort_index()\n",
        "\n",
        "    # Schema\n",
        "    schema = _schema_and_missing_table(df)\n",
        "    schema = _inject_index_as_column(schema)\n",
        "    figs.append(_table_figure_from_dataframe(schema, f\"{name} • Schema & Missingness\"))\n",
        "\n",
        "    # Numeric Quality\n",
        "    quality = _numeric_quality_and_ranges(df)\n",
        "    quality = _inject_index_as_column(quality)\n",
        "    figs.append(_table_figure_from_dataframe(quality, f\"{name} • Numeric Quality & Ranges\"))\n",
        "\n",
        "    # Describe\n",
        "    num = _select_numeric_cols(df)\n",
        "    if not num.empty:\n",
        "        desc = num.describe(percentiles=[0.01,0.05,0.25,0.5,0.75,0.95,0.99]).T\n",
        "        desc = _inject_index_as_column(desc)\n",
        "        figs.append(_table_figure_from_dataframe(desc, f\"{name} • Describe\"))\n",
        "    else:\n",
        "        figs.append(_table_figure_from_dataframe(\n",
        "            pd.DataFrame({\"Message\":[\"No numeric columns\"]}),\n",
        "            f\"{name} • Describe\"\n",
        "        ))\n",
        "\n",
        "    # Correlation\n",
        "    figs.append(_corr_heatmap(df, f\"{name} • Correlation\"))\n",
        "\n",
        "    # Price Features\n",
        "    close_col = _infer_close_col(df)\n",
        "    if close_col:\n",
        "        price = df[close_col].astype(float)\n",
        "\n",
        "        # Price Plot\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.plot(price.index, price.values)\n",
        "        ax.set_title(f\"{name} • {close_col} Price\")\n",
        "        figs.append(fig)\n",
        "\n",
        "        # Rolling Vol\n",
        "        vol = _rolling_volatility(price)\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.plot(vol.index, vol.values, color=\"darkorange\")\n",
        "        ax.set_title(f\"{name} • 30-Day Rolling Volatility\")\n",
        "        figs.append(fig)\n",
        "\n",
        "        # Return Distribution\n",
        "        rets = np.log(price).diff().dropna()\n",
        "        fig, ax = plt.subplots()\n",
        "        sns.histplot(rets, bins=60, kde=True, color=\"teal\")\n",
        "        ax.set_title(f\"{name} • Log Return Distribution\")\n",
        "        figs.append(fig)\n",
        "\n",
        "    return figs\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# Cross-Ticker EDA\n",
        "# --------------------------------------\n",
        "def eda_cross_ticker(frames: Dict[str, pd.DataFrame]) -> List[plt.Figure]:\n",
        "    figs = []\n",
        "    price_map = {}\n",
        "\n",
        "    for name, df in frames.items():\n",
        "        col = _infer_close_col(df)\n",
        "        if col:\n",
        "            price_map[name] = df[col].astype(float)\n",
        "\n",
        "    if price_map:\n",
        "        prices = pd.DataFrame(price_map).sort_index()\n",
        "\n",
        "        # Price Overlay\n",
        "        fig, ax = plt.subplots()\n",
        "        for name in prices.columns:\n",
        "            ax.plot(prices.index, prices[name], lw=1.2, label=name)\n",
        "        ax.legend()\n",
        "        ax.set_title(\"Cross-Ticker • Close Prices\")\n",
        "        figs.append(fig)\n",
        "\n",
        "        # Return Correlation\n",
        "        rets = np.log(prices).diff().dropna()\n",
        "        fig, ax = plt.subplots()\n",
        "        sns.heatmap(rets.corr(), annot=True, cmap=\"viridis\")\n",
        "        ax.set_title(\"Cross-Ticker • Log Return Corr\")\n",
        "        figs.append(fig)\n",
        "\n",
        "    return figs\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# Build PDF Report\n",
        "# --------------------------------------\n",
        "def build_engineered_feature_eda(\n",
        "    frames: Dict[str, pd.DataFrame],\n",
        "    pdf_title: str = \"Engineered Feature EDA.pdf\",\n",
        "    show_plots: bool = False\n",
        "):\n",
        "    figs = []\n",
        "    ordered = sorted(frames.items(), key=lambda x: x[0])\n",
        "\n",
        "    for name, df in ordered:\n",
        "        figs.extend(eda_single_ticker(name, df))\n",
        "\n",
        "    figs.extend(eda_cross_ticker(frames))\n",
        "\n",
        "    with PdfPages(pdf_title) as pdf:\n",
        "\n",
        "        cover, ax = plt.subplots()\n",
        "        ax.axis(\"off\")\n",
        "        ax.text(0.02, 0.92, \"Engineered Feature EDA Report\", fontsize=22, weight=\"bold\")\n",
        "        ax.text(0.02, 0.84, \"Tickers: \" + \", \".join(frames.keys()), fontsize=12)\n",
        "\n",
        "        lines = [\n",
        "            \"• Schema & Missingness\",\n",
        "            \"• Numeric Quality & Ranges\",\n",
        "            \"• Numeric Overview\",\n",
        "            \"• Correlation Heatmap\",\n",
        "            \"• Price Trend & Volatility\",\n",
        "            \"• Log Return Distribution\",\n",
        "            \"• Cross-Ticker Price Overlay\",\n",
        "            \"• Cross-Ticker Return Correlation\"\n",
        "        ]\n",
        "\n",
        "        for i, l in enumerate(lines):\n",
        "            ax.text(0.03, 0.75 - 0.05*i, l, fontsize=11)\n",
        "\n",
        "        pdf.savefig(cover)\n",
        "        plt.close(cover)\n",
        "\n",
        "        for fig in figs:\n",
        "            pdf.savefig(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "    print(f\"📄 PDF saved: {pdf_title}\")\n",
        "\n",
        "    if show_plots:\n",
        "        for fig in figs:\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# Example Usage\n",
        "# --------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Already-loaded engineered DataFrames\n",
        "    frames = {\n",
        "        \"SPY\": spy_df,\n",
        "        \"GLD\": gld_df,\n",
        "        \"CL=F\": clf_df,\n",
        "        \"PSX\": psx_df,\n",
        "        \"VLO\": vlo_df,\n",
        "        \"MPC\": mpc_df,\n",
        "        \"EUR_USD\": eurusd_df\n",
        "    }\n",
        "\n",
        "    build_engineered_feature_eda(frames, pdf_title=\"Engineered Feature GARCH EDA.pdf\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/tmp/ipykernel_11276/2448325007.py:234: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n  fig, ax = plt.subplots()\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:395: RuntimeWarning: invalid value encountered in log\n  result = func(self.values, **kwargs)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "📄 PDF saved: Engineered Feature GARCH EDA.pdf\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1771455245904
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify Financial Market Stylized facts"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Multi-Asset Plotting Suite — Stylized Facts & Seasonality (Single-Ticker Frames)\n",
        "\n",
        "Updates in this version:\n",
        "- Uses your new frames/tickers:\n",
        "    \"SPY\": spy_df,\n",
        "    \"GLD\": gld_df,\n",
        "    \"CL=F\": clf_df,\n",
        "    \"PSX\": psx_df,\n",
        "    \"VLO\": vlo_df,\n",
        "    \"MPC\": mpc_df,\n",
        "    \"EUR_USD\": eurusd_df\n",
        "- More robust column detection for single-ticker frames:\n",
        "    Works with either:\n",
        "      \"Adj Close|TICKER\" / \"Close|TICKER\"   (engineered multi-asset naming)\n",
        "    OR\n",
        "      \"Adj Close\" / \"Close\"                (plain single-ticker naming)\n",
        "    Same idea for \"log_ret|TICKER\" vs \"log_ret\".\n",
        "- Regex feature detection now safely handles tickers like \"CL=F\" via re.escape().\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from typing import Dict, List, Optional\n",
        "from scipy import stats\n",
        "\n",
        "# ----------------------------\n",
        "# Plot styling\n",
        "# ----------------------------\n",
        "plt.rcParams[\"figure.figsize\"] = (11, 7)\n",
        "plt.rcParams[\"axes.grid\"] = True\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def _safe_title(name: str) -> str:\n",
        "    return str(name).strip() if name else \"Unknown\"\n",
        "\n",
        "def _table_figure_from_dataframe(df: pd.DataFrame, title: str, font_size: int = 9) -> plt.Figure:\n",
        "    fig, ax = plt.subplots(figsize=(11, min(7, 1 + 0.35 * (len(df) + 2))))\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(title, fontsize=14, pad=12, loc=\"left\")\n",
        "    tbl = ax.table(\n",
        "        cellText=df.round(4).values.tolist(),\n",
        "        colLabels=[str(c) for c in df.columns],\n",
        "        loc=\"center\",\n",
        "        cellLoc=\"right\",\n",
        "    )\n",
        "    tbl.auto_set_font_size(False)\n",
        "    tbl.set_fontsize(font_size)\n",
        "    tbl.scale(1, 1.2)\n",
        "    return fig\n",
        "\n",
        "def _numeric_clean(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"Coerce to numeric and drop NaNs.\"\"\"\n",
        "    return pd.to_numeric(series, errors=\"coerce\").dropna()\n",
        "\n",
        "def _find_col_case_insensitive(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
        "    \"\"\"Return the first matching column from candidates (case-insensitive), else None.\"\"\"\n",
        "    low = {str(c).lower(): c for c in df.columns}\n",
        "    for cand in candidates:\n",
        "        hit = low.get(str(cand).lower())\n",
        "        if hit is not None:\n",
        "            return hit\n",
        "    return None\n",
        "\n",
        "def _adj_or_close_col(df: pd.DataFrame, ticker: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Prefer adjusted close; support both:\n",
        "      - \"Adj Close|TICKER\" / \"Close|TICKER\"\n",
        "      - \"Adj Close\" / \"Close\"\n",
        "    Also tries common alternates like \"AdjClose\", \"Adj_Close\", etc.\n",
        "    \"\"\"\n",
        "    candidates = [\n",
        "        f\"Adj Close|{ticker}\", f\"Close|{ticker}\",\n",
        "        \"Adj Close\", \"Close\",\n",
        "        f\"AdjClose|{ticker}\", f\"Adj_Close|{ticker}\", f\"Adjusted Close|{ticker}\",\n",
        "        \"AdjClose\", \"Adj_Close\", \"Adjusted Close\",\n",
        "    ]\n",
        "    return _find_col_case_insensitive(df, candidates)\n",
        "\n",
        "# ----------------------------\n",
        "# Detect engineered feature columns\n",
        "# ----------------------------\n",
        "def _feature_cols_for_ticker(df: pd.DataFrame, ticker: str) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Supports both engineered multi-asset naming:\n",
        "        HL_spread|TICKER, SMA_20|TICKER, lag_1|TICKER\n",
        "    and plain single-ticker naming:\n",
        "        HL_spread, SMA_20, lag_1\n",
        "    \"\"\"\n",
        "    t = re.escape(ticker)\n",
        "\n",
        "    # engineered patterns (with |TICKER)\n",
        "    spreads_pat_t = re.compile(rf\"^(HL_spread|OC_spread)\\|{t}$\")\n",
        "    ma_pat_t      = re.compile(rf\"^(SMA_\\d+|EMA_\\d+)\\|{t}$\")\n",
        "    lag_pat_t     = re.compile(rf\"^lag_\\d+\\|{t}$\")\n",
        "\n",
        "    # plain patterns (no suffix)\n",
        "    spreads_pat   = re.compile(r\"^(HL_spread|OC_spread)$\")\n",
        "    ma_pat        = re.compile(r\"^(SMA_\\d+|EMA_\\d+)$\")\n",
        "    lag_pat       = re.compile(r\"^lag_\\d+$\")\n",
        "\n",
        "    cols = [str(c) for c in df.columns]\n",
        "    spreads = [c for c in cols if spreads_pat_t.match(c) or spreads_pat.match(c)]\n",
        "    ma      = [c for c in cols if ma_pat_t.match(c)      or ma_pat.match(c)]\n",
        "    lags    = [c for c in cols if lag_pat_t.match(c)     or lag_pat.match(c)]\n",
        "\n",
        "    return {\"spreads\": spreads, \"ma\": ma, \"lags\": lags}\n",
        "\n",
        "# ----------------------------\n",
        "# Return series aggregator\n",
        "# ----------------------------\n",
        "def _get_return_series(df: pd.DataFrame, ticker: str) -> Optional[pd.Series]:\n",
        "    \"\"\"\n",
        "    Returns daily log returns from engineered column or derives from price.\n",
        "\n",
        "    Supports both:\n",
        "      - \"log_ret|TICKER\" or \"log_ret\"\n",
        "      - and derivation from price column(s)\n",
        "    \"\"\"\n",
        "    # engineered return col\n",
        "    ret_col = _find_col_case_insensitive(df, [f\"log_ret|{ticker}\", \"log_ret\"])\n",
        "    if ret_col is not None:\n",
        "        s = _numeric_clean(df[ret_col]).sort_index()\n",
        "        return s if not s.empty else None\n",
        "\n",
        "    # derive from price\n",
        "    price_col = _adj_or_close_col(df, ticker)\n",
        "    if price_col:\n",
        "        p = _numeric_clean(df[price_col]).sort_index()\n",
        "        if not p.empty:\n",
        "            return np.log(p / p.shift(1)).dropna()\n",
        "\n",
        "    return None\n",
        "\n",
        "# ----------------------------\n",
        "# Core Plot Helpers (stylized facts)\n",
        "# ----------------------------\n",
        "def _plot_price_series(price: pd.Series, ticker: str) -> plt.Figure:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(price.index, price.values, label=\"Adj/Close\", color=\"royalblue\", lw=1.6)\n",
        "    ax.set_title(f\"{ticker} • Price/Level\")\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    ax.set_ylabel(\"Price / Level\")\n",
        "    ax.legend(loc=\"best\")\n",
        "    return fig\n",
        "\n",
        "def _plot_price_acf(price: pd.Series, ticker: str, max_lags: int = 20) -> plt.Figure:\n",
        "    title = f\"{ticker} • ACF • Price/Level\"\n",
        "    try:\n",
        "        from statsmodels.graphics.tsaplots import plot_acf\n",
        "        fig = plot_acf(price.dropna(), lags=max_lags)\n",
        "        fig.axes[0].set_title(title)\n",
        "        return fig\n",
        "    except Exception:\n",
        "        x = price.dropna()\n",
        "        lags = list(range(1, max_lags + 1))\n",
        "        values = [x.corr(x.shift(k)) for k in lags]\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.bar(lags, values, color=\"slategray\")\n",
        "        ax.axhline(0, color=\"gray\", lw=0.8)\n",
        "        ax.set_title(f\"{title} (manual)\")\n",
        "        ax.set_xlabel(\"Lag\")\n",
        "        ax.set_ylabel(\"Autocorrelation\")\n",
        "        return fig\n",
        "\n",
        "def _plot_mas(price: pd.Series, ticker: str) -> plt.Figure:\n",
        "    s = price.sort_index()\n",
        "    ma_5  = s.rolling(window=5,  min_periods=5).mean()\n",
        "    ma_10 = s.rolling(window=10, min_periods=10).mean()\n",
        "    ma_20 = s.rolling(window=20, min_periods=20).mean()\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(s.index, s.values, color=\"royalblue\", lw=1.0, alpha=0.6, label=\"Adj/Close\")\n",
        "    ax.plot(ma_5.index, ma_5.values, color=\"darkorange\", lw=1.8, label=\"SMA 5\")\n",
        "    ax.plot(ma_10.index, ma_10.values, color=\"seagreen\", lw=1.8, label=\"SMA 10\")\n",
        "    ax.plot(ma_20.index, ma_20.values, color=\"crimson\", lw=1.8, label=\"SMA 20\")\n",
        "    ax.set_title(f\"{ticker} • Moving Averages (5/10/20)\")\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    ax.set_ylabel(\"Price / Level\")\n",
        "    ax.legend(loc=\"best\")\n",
        "    return fig\n",
        "\n",
        "def _plot_returns_ts(r: pd.Series, ticker: str) -> plt.Figure:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(r.index, r.values, color=\"darkorange\", lw=1.2)\n",
        "    ax.axhline(0.0, color=\"gray\", lw=0.8)\n",
        "    ax.set_title(f\"{ticker} • Daily Log Returns\")\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    ax.set_ylabel(\"Daily log return\")\n",
        "    return fig\n",
        "\n",
        "def _plot_returns_hist_qq(r: pd.Series, ticker: str) -> List[plt.Figure]:\n",
        "    \"\"\"Histogram + KDE with stats box (mean/median/mode/std/skew/kurt) and a Q–Q plot.\"\"\"\n",
        "    figs: List[plt.Figure] = []\n",
        "\n",
        "    # Histogram + KDE + stats box\n",
        "    fig_hist, ax_hist = plt.subplots()\n",
        "    sns.histplot(r, bins=60, kde=True, stat=\"density\", color=\"teal\", ax=ax_hist)\n",
        "    ax_hist.set_title(f\"{ticker} • Returns • Distribution\")\n",
        "    ax_hist.set_xlabel(\"Daily log return\")\n",
        "    ax_hist.set_ylabel(\"Density\")\n",
        "\n",
        "    r_clean = _numeric_clean(r)\n",
        "    mean   = float(r_clean.mean()) if len(r_clean) else np.nan\n",
        "    median = float(r_clean.median()) if len(r_clean) else np.nan\n",
        "    mode_series = r_clean.mode()\n",
        "    mode_val = float(mode_series.iloc[0]) if not mode_series.empty else np.nan\n",
        "    std    = float(r_clean.std(ddof=1)) if len(r_clean) else np.nan\n",
        "    skew   = float(stats.skew(r_clean, bias=False, nan_policy=\"omit\")) if len(r_clean) else np.nan\n",
        "    kurt   = float(stats.kurtosis(r_clean, fisher=True, bias=False, nan_policy=\"omit\")) if len(r_clean) else np.nan\n",
        "\n",
        "    stats_text = \"\\n\".join([\n",
        "        f\"Mean:     {mean:.6f}\"     if not np.isnan(mean) else \"Mean:     n/a\",\n",
        "        f\"Median:   {median:.6f}\"   if not np.isnan(median) else \"Median:   n/a\",\n",
        "        f\"Mode:     {mode_val:.6f}\" if not np.isnan(mode_val) else \"Mode:     n/a\",\n",
        "        f\"Std Dev:  {std:.6f}\"      if not np.isnan(std) else \"Std Dev:  n/a\",\n",
        "        f\"Skewness: {skew:.4f}\"     if not np.isnan(skew) else \"Skewness: n/a\",\n",
        "        f\"Kurtosis: {kurt:.4f}\"     if not np.isnan(kurt) else \"Kurtosis: n/a\",\n",
        "    ])\n",
        "    ax_hist.text(\n",
        "        0.02, 0.98, stats_text,\n",
        "        transform=ax_hist.transAxes,\n",
        "        fontsize=10,\n",
        "        va=\"top\", ha=\"left\",\n",
        "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.85, edgecolor=\"gray\")\n",
        "    )\n",
        "\n",
        "    # Guide lines: mean and ±1σ\n",
        "    if not np.isnan(mean):\n",
        "        ax_hist.axvline(mean, color=\"black\", lw=1.4, linestyle=\"--\", label=\"Mean\")\n",
        "    if not (np.isnan(mean) or np.isnan(std)):\n",
        "        ax_hist.axvline(mean - std, color=\"gray\", lw=1.0, linestyle=\"--\", label=\"±1σ\")\n",
        "        ax_hist.axvline(mean + std, color=\"gray\", lw=1.0, linestyle=\"--\")\n",
        "        handles, labels = ax_hist.get_legend_handles_labels()\n",
        "        if labels:\n",
        "            ax_hist.legend(loc=\"best\", frameon=True)\n",
        "\n",
        "    figs.append(fig_hist)\n",
        "\n",
        "    # Q–Q plot\n",
        "    fig_qq, ax_qq = plt.subplots()\n",
        "    stats.probplot(r_clean, dist=\"norm\", plot=ax_qq)\n",
        "    ax_qq.set_title(f\"{ticker} • Returns • Q–Q Plot vs Normal\")\n",
        "    figs.append(fig_qq)\n",
        "\n",
        "    return figs\n",
        "\n",
        "def _plot_returns_acf(r: pd.Series, ticker: str, max_lags: int = 20) -> plt.Figure:\n",
        "    title = f\"{ticker} • ACF • Returns\"\n",
        "    try:\n",
        "        from statsmodels.graphics.tsaplots import plot_acf\n",
        "        fig = plot_acf(r.dropna(), lags=max_lags)\n",
        "        fig.axes[0].set_title(title)\n",
        "        return fig\n",
        "    except Exception:\n",
        "        fig, ax = plt.subplots()\n",
        "        lags = list(range(1, max_lags + 1))\n",
        "        values = [r.corr(r.shift(k)) for k in lags]\n",
        "        ax.bar(lags, values, color=\"steelblue\")\n",
        "        ax.axhline(0, color=\"gray\", lw=0.8)\n",
        "        ax.set_title(f\"{title} (manual)\")\n",
        "        ax.set_xlabel(\"Lag\")\n",
        "        ax.set_ylabel(\"Autocorrelation\")\n",
        "        return fig\n",
        "\n",
        "def _plot_rolling_autocorr(r: pd.Series, ticker: str, lag: int = 1, window: int = 20) -> plt.Figure:\n",
        "    df_tmp = pd.DataFrame({\"r\": r}).dropna()\n",
        "    df_tmp[f\"r_lag_{lag}\"] = df_tmp[\"r\"].shift(lag)\n",
        "    roll = df_tmp[\"r\"].rolling(window).corr(df_tmp[f\"r_lag_{lag}\"])\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(roll.index, roll.values, color=\"forestgreen\", lw=1.2)\n",
        "    ax.axhline(0, color=\"gray\", lw=0.8)\n",
        "    ax.set_title(f\"{ticker} • Rolling Autocorrelation (lag={lag}, window={window})\")\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    ax.set_ylabel(\"Autocorr\")\n",
        "    return fig\n",
        "\n",
        "def _plot_vol_clustering(r: pd.Series, ticker: str, max_lags: int = 20) -> List[plt.Figure]:\n",
        "    \"\"\"ACF of |r| and r^2 to reveal volatility clustering.\"\"\"\n",
        "    figs: List[plt.Figure] = []\n",
        "    abs_r = r.abs()\n",
        "    sq_r  = r.pow(2)\n",
        "\n",
        "    # ACF |r|\n",
        "    try:\n",
        "        from statsmodels.graphics.tsaplots import plot_acf\n",
        "        fig1 = plot_acf(abs_r.dropna(), lags=max_lags)\n",
        "        fig1.axes[0].set_title(f\"{ticker} • ACF • |Returns| (Volatility Clustering)\")\n",
        "        figs.append(fig1)\n",
        "    except Exception:\n",
        "        lags = list(range(1, max_lags + 1))\n",
        "        values = [abs_r.corr(abs_r.shift(k)) for k in lags]\n",
        "        fig1, ax1 = plt.subplots()\n",
        "        ax1.bar(lags, values, color=\"indianred\")\n",
        "        ax1.axhline(0, color=\"gray\", lw=0.8)\n",
        "        ax1.set_title(f\"{ticker} • ACF • |Returns| (manual)\")\n",
        "        figs.append(fig1)\n",
        "\n",
        "    # ACF r^2\n",
        "    try:\n",
        "        from statsmodels.graphics.tsaplots import plot_acf\n",
        "        fig2 = plot_acf(sq_r.dropna(), lags=max_lags)\n",
        "        fig2.axes[0].set_title(f\"{ticker} • ACF • Returns^2 (Volatility Clustering)\")\n",
        "        figs.append(fig2)\n",
        "    except Exception:\n",
        "        lags = list(range(1, max_lags + 1))\n",
        "        values = [sq_r.corr(sq_r.shift(k)) for k in lags]\n",
        "        fig2, ax2 = plt.subplots()\n",
        "        ax2.bar(lags, values, color=\"firebrick\")\n",
        "        ax2.axhline(0, color=\"gray\", lw=0.8)\n",
        "        ax2.set_title(f\"{ticker} • ACF • Returns^2 (manual)\")\n",
        "        figs.append(fig2)\n",
        "\n",
        "    return figs\n",
        "\n",
        "# ----------------------------\n",
        "# Seasonality (derived from Date index)\n",
        "# ----------------------------\n",
        "def _seasonal_df_from_index(index_like: pd.Index) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Build seasonal fields from a Date-like index. Returns None if cannot coerce.\"\"\"\n",
        "    dt = pd.to_datetime(index_like, errors=\"coerce\")\n",
        "    if dt.isna().all():\n",
        "        return None\n",
        "    dt = pd.DatetimeIndex(dt)\n",
        "    out = pd.DataFrame(\n",
        "        {\n",
        "            \"month\": dt.month,\n",
        "            \"day_of_week\": dt.dayofweek,  # 0=Mon .. 6=Sun\n",
        "            \"quarter\": dt.quarter,\n",
        "            \"year\": dt.year,\n",
        "            \"is_month_end\": dt.is_month_end.astype(int),\n",
        "            \"is_month_start\": dt.is_month_start.astype(int),\n",
        "        },\n",
        "        index=index_like,\n",
        "    )\n",
        "    return out\n",
        "\n",
        "def _plot_monthly_box_from_index(r: pd.Series, ticker: str) -> Optional[plt.Figure]:\n",
        "    season = _seasonal_df_from_index(r.index)\n",
        "    if season is None:\n",
        "        return None\n",
        "    d = pd.concat([r.rename(\"ret\"), season[\"month\"]], axis=1).dropna(subset=[\"ret\"])\n",
        "    d[\"month\"] = pd.to_numeric(d[\"month\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    d = d.dropna(subset=[\"month\"])\n",
        "    if d.empty:\n",
        "        return None\n",
        "\n",
        "    order = list(range(1, 13))\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.boxplot(data=d, x=\"month\", y=\"ret\", ax=ax, order=order)\n",
        "    ax.set_title(f\"{ticker} • Monthly Returns\")\n",
        "    ax.set_xlabel(\"Month (1–12)\")\n",
        "    ax.set_ylabel(\"Daily log return\")\n",
        "\n",
        "    # Overlay mean by month + annotations\n",
        "    grp = d.groupby(\"month\")[\"ret\"].mean()\n",
        "    x_positions = ax.get_xticks()\n",
        "    means_in_order = [float(grp.get(m, np.nan)) for m in order]\n",
        "\n",
        "    ax.plot(x_positions, means_in_order, color=\"crimson\", marker=\"D\", lw=1.6,\n",
        "            label=\"Average (mean)\", zorder=5)\n",
        "\n",
        "    for x, y in zip(x_positions, means_in_order):\n",
        "        if not (np.isnan(y) or np.isinf(y)):\n",
        "            ax.annotate(f\"{y:+.4f}\", (x, y),\n",
        "                        textcoords=\"offset points\", xytext=(0, 8),\n",
        "                        ha=\"center\", fontsize=9, color=\"crimson\", fontweight=\"bold\")\n",
        "\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    if \"Average (mean)\" in labels:\n",
        "        ax.legend(loc=\"best\", frameon=True)\n",
        "\n",
        "    return fig\n",
        "\n",
        "def _plot_dow_box_from_index(r: pd.Series, ticker: str) -> Optional[plt.Figure]:\n",
        "    season = _seasonal_df_from_index(r.index)\n",
        "    if season is None:\n",
        "        return None\n",
        "    d = pd.concat([r.rename(\"ret\"), season[\"day_of_week\"]], axis=1).dropna(subset=[\"ret\"])\n",
        "    d[\"day_of_week\"] = pd.to_numeric(d[\"day_of_week\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    d = d.dropna(subset=[\"day_of_week\"])\n",
        "    if d.empty:\n",
        "        return None\n",
        "\n",
        "    labels = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
        "    order = list(range(0, 7))\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.boxplot(data=d, x=\"day_of_week\", y=\"ret\", ax=ax, order=order)\n",
        "    ax.set_title(f\"{ticker} • Day-of-Week Returns\")\n",
        "    ax.set_xlabel(\"Day of Week\")\n",
        "    ax.set_xticklabels(labels, rotation=0)\n",
        "    ax.set_ylabel(\"Daily log return\")\n",
        "\n",
        "    # Overlay mean by DOW + annotations\n",
        "    grp = d.groupby(\"day_of_week\")[\"ret\"].mean()\n",
        "    x_positions = ax.get_xticks()\n",
        "    means_in_order = [float(grp.get(i, np.nan)) for i in order]\n",
        "\n",
        "    ax.plot(x_positions, means_in_order, color=\"crimson\", marker=\"D\", lw=1.6,\n",
        "            label=\"Average (mean)\", zorder=5)\n",
        "\n",
        "    for x, y in zip(x_positions, means_in_order):\n",
        "        if not (np.isnan(y) or np.isinf(y)):\n",
        "            ax.annotate(f\"{y:+.4f}\", (x, y),\n",
        "                        textcoords=\"offset points\", xytext=(0, 8),\n",
        "                        ha=\"center\", fontsize=9, color=\"crimson\", fontweight=\"bold\")\n",
        "\n",
        "    handles, labels_ = ax.get_legend_handles_labels()\n",
        "    if \"Average (mean)\" in labels_:\n",
        "        ax.legend(loc=\"best\", frameon=True)\n",
        "\n",
        "    return fig\n",
        "\n",
        "def _plot_quarter_box_from_index(r: pd.Series, ticker: str) -> Optional[plt.Figure]:\n",
        "    season = _seasonal_df_from_index(r.index)\n",
        "    if season is None:\n",
        "        return None\n",
        "    d = pd.concat([r.rename(\"ret\"), season[\"quarter\"]], axis=1).dropna(subset=[\"ret\"])\n",
        "    d[\"quarter\"] = pd.to_numeric(d[\"quarter\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    d = d.dropna(subset=[\"quarter\"])\n",
        "    if d.empty:\n",
        "        return None\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.boxplot(data=d, x=\"quarter\", y=\"ret\", ax=ax, order=[1, 2, 3, 4])\n",
        "    ax.set_title(f\"{ticker} • Quarterly Returns\")\n",
        "    ax.set_xlabel(\"Quarter\")\n",
        "    ax.set_ylabel(\"Daily log return\")\n",
        "    return fig\n",
        "\n",
        "def _plot_month_year_heatmap_from_index(r: pd.Series, ticker: str) -> Optional[plt.Figure]:\n",
        "    season = _seasonal_df_from_index(r.index)\n",
        "    if season is None:\n",
        "        return None\n",
        "    d = pd.concat([r.rename(\"ret\"), season[[\"month\", \"year\"]]], axis=1).dropna(subset=[\"ret\"])\n",
        "    d[\"month\"] = pd.to_numeric(d[\"month\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    d[\"year\"]  = pd.to_numeric(d[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    d = d.dropna(subset=[\"month\", \"year\"])\n",
        "    if d.empty:\n",
        "        return None\n",
        "    piv = d.pivot_table(index=\"year\", columns=\"month\", values=\"ret\", aggfunc=\"mean\").sort_index()\n",
        "    piv = piv.reindex(columns=list(range(1, 13)))\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(piv, cmap=\"RdYlGn\", center=0, annot=False, ax=ax)\n",
        "    ax.set_title(f\"{ticker} • Month×Year Heatmap (Avg Daily Returns)\")\n",
        "    ax.set_xlabel(\"Month\")\n",
        "    ax.set_ylabel(\"Year\")\n",
        "    return fig\n",
        "\n",
        "def _plot_month_end_start_bars_from_index(r: pd.Series, ticker: str) -> Optional[plt.Figure]:\n",
        "    season = _seasonal_df_from_index(r.index)\n",
        "    if season is None:\n",
        "        return None\n",
        "    d = pd.concat([r.rename(\"ret\"), season[[\"is_month_end\", \"is_month_start\"]]], axis=1).dropna(subset=[\"ret\"])\n",
        "    d[\"is_month_end\"]   = pd.to_numeric(d[\"is_month_end\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    d[\"is_month_start\"] = pd.to_numeric(d[\"is_month_start\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    d = d.dropna(subset=[\"is_month_end\", \"is_month_start\"])\n",
        "    if d.empty:\n",
        "        return None\n",
        "\n",
        "    rows = []\n",
        "    for col, label in [(\"is_month_end\", \"Month End\"), (\"is_month_start\", \"Month Start\")]:\n",
        "        grp = d.groupby(col)[\"ret\"].mean()\n",
        "        rows.append({\"Group\": f\"{label}:Yes\", \"AvgRet\": float(grp.get(1, np.nan))})\n",
        "        rows.append({\"Group\": f\"{label}:No\",  \"AvgRet\": float(grp.get(0, np.nan))})\n",
        "    res = pd.DataFrame(rows)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.barplot(data=res, x=\"Group\", y=\"AvgRet\", palette=\"Blues_d\", ax=ax)\n",
        "    ax.axhline(0, color=\"gray\", lw=0.8)\n",
        "    ax.set_title(f\"{ticker} • Avg Returns: Month-End/Start vs Others\")\n",
        "    ax.set_xlabel(\"\")\n",
        "    ax.set_ylabel(\"Average Daily Log Return\")\n",
        "    ax.tick_params(axis=\"x\", rotation=15)\n",
        "    return fig\n",
        "\n",
        "# ----------------------------\n",
        "# Engineered Feature Visuals\n",
        "# ----------------------------\n",
        "def _plot_feature_distributions(df: pd.DataFrame, cols: List[str], title_prefix: str, ticker: str) -> List[plt.Figure]:\n",
        "    figs: List[plt.Figure] = []\n",
        "    cols_num = [c for c in cols if c in df.columns and _numeric_clean(df[c]).size > 0]\n",
        "    if not cols_num:\n",
        "        return figs\n",
        "    chunk_size = 6\n",
        "    for i in range(0, len(cols_num), chunk_size):\n",
        "        batch = cols_num[i:i + chunk_size]\n",
        "        rows = int(np.ceil(len(batch) / 3))\n",
        "        fig, axes = plt.subplots(rows, 3, figsize=(11, max(3.5, 3.2 * rows)))\n",
        "        axes = np.array(axes).reshape(-1)\n",
        "        for j, c in enumerate(batch):\n",
        "            s = _numeric_clean(df[c])\n",
        "            sns.histplot(s, bins=50, kde=True, ax=axes[j], color=\"teal\")\n",
        "            axes[j].set_title(c)\n",
        "        for k in range(len(batch), len(axes)):\n",
        "            fig.delaxes(axes[k])\n",
        "        fig.suptitle(f\"{ticker} • {title_prefix}\", x=0.06, y=0.99, ha=\"left\", fontsize=14)\n",
        "        figs.append(fig)\n",
        "    return figs\n",
        "\n",
        "def _plot_feature_corr(df: pd.DataFrame, cols: List[str], title: str, ticker: str) -> Optional[plt.Figure]:\n",
        "    cols_num = [c for c in cols if c in df.columns and _numeric_clean(df[c]).size > 0]\n",
        "    if len(cols_num) < 2:\n",
        "        return None\n",
        "    data = df[cols_num].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    corr = data.corr()\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(corr, cmap=\"viridis\", center=0, annot=True, fmt=\".2f\", ax=ax)\n",
        "    ax.set_title(f\"{ticker} • {title}\", loc=\"left\")\n",
        "    return fig\n",
        "\n",
        "# ----------------------------\n",
        "# Per-ticker report\n",
        "# ----------------------------\n",
        "def eda_single_ticker(name: str, df: pd.DataFrame) -> List[plt.Figure]:\n",
        "    figs: List[plt.Figure] = []\n",
        "    ticker = _safe_title(name)\n",
        "    df = df.copy().sort_index()\n",
        "\n",
        "    # Price series\n",
        "    price_col = _adj_or_close_col(df, ticker)\n",
        "    if price_col:\n",
        "        price = _numeric_clean(df[price_col])\n",
        "        if not price.empty:\n",
        "            figs.append(_plot_price_series(price, ticker))\n",
        "            figs.append(_plot_price_acf(price, ticker, max_lags=20))\n",
        "            figs.append(_plot_mas(price, ticker))\n",
        "    else:\n",
        "        info_df = pd.DataFrame({\"Message\": [f\"No Adj/Close column found for {ticker}.\"]})\n",
        "        figs.append(_table_figure_from_dataframe(info_df, f\"{ticker} • Price Not Available\"))\n",
        "\n",
        "    # Returns-driven stylized facts\n",
        "    r = _get_return_series(df, ticker)\n",
        "    if r is not None and not r.empty:\n",
        "        figs.append(_plot_returns_ts(r, ticker))\n",
        "        figs.extend(_plot_returns_hist_qq(r, ticker))\n",
        "        figs.append(_plot_returns_acf(r, ticker, max_lags=20))\n",
        "        figs.append(_plot_rolling_autocorr(r, ticker, lag=1, window=20))\n",
        "        figs.extend(_plot_vol_clustering(r, ticker, max_lags=20))\n",
        "\n",
        "        # Seasonality — derived from the Date index of r\n",
        "        fig_m = _plot_monthly_box_from_index(r, ticker);          figs.append(fig_m) if fig_m else None\n",
        "        fig_d = _plot_dow_box_from_index(r, ticker);              figs.append(fig_d) if fig_d else None\n",
        "        fig_q = _plot_quarter_box_from_index(r, ticker);          figs.append(fig_q) if fig_q else None\n",
        "        fig_h = _plot_month_year_heatmap_from_index(r, ticker);   figs.append(fig_h) if fig_h else None\n",
        "        fig_b = _plot_month_end_start_bars_from_index(r, ticker); figs.append(fig_b) if fig_b else None\n",
        "    else:\n",
        "        info_df = pd.DataFrame({\"Message\": [f\"No return series found for {ticker}.\"]})\n",
        "        figs.append(_table_figure_from_dataframe(info_df, f\"{ticker} • Returns Not Available\"))\n",
        "\n",
        "    # Engineered feature plots\n",
        "    feat = _feature_cols_for_ticker(df, ticker)\n",
        "    figs.extend(_plot_feature_distributions(df, feat[\"spreads\"], \"Spreads\", ticker))\n",
        "    figs.extend(_plot_feature_distributions(df, feat[\"ma\"], \"Moving Averages / EMAs\", ticker))\n",
        "    figs.extend(_plot_feature_distributions(df, feat[\"lags\"], \"Lagged Prices\", ticker))\n",
        "\n",
        "    fm = _plot_feature_corr(df, feat[\"ma\"], \"Correlation • Moving Averages\", ticker)\n",
        "    if fm:\n",
        "        figs.append(fm)\n",
        "    fm2 = _plot_feature_corr(df, feat[\"spreads\"] + feat[\"lags\"], \"Correlation • Spreads + Lags\", ticker)\n",
        "    if fm2:\n",
        "        figs.append(fm2)\n",
        "\n",
        "    return figs\n",
        "\n",
        "# ----------------------------\n",
        "# Cross-ticker overlay (Adj/Close)\n",
        "# ----------------------------\n",
        "def eda_cross_ticker_adjclose(frames: Dict[str, pd.DataFrame]) -> List[plt.Figure]:\n",
        "    figs: List[plt.Figure] = []\n",
        "    price_cols: Dict[str, pd.Series] = {}\n",
        "\n",
        "    for name, df in frames.items():\n",
        "        t = _safe_title(name)\n",
        "        price_col = _adj_or_close_col(df, t)\n",
        "        if price_col:\n",
        "            series = _numeric_clean(df[price_col])\n",
        "            if not series.empty:\n",
        "                price_cols[t] = series\n",
        "\n",
        "    if price_cols:\n",
        "        prices = pd.DataFrame(price_cols).sort_index()\n",
        "        fig, ax = plt.subplots()\n",
        "        for name in prices.columns:\n",
        "            ax.plot(prices.index, prices[name], lw=1.5, label=name)\n",
        "        ax.set_title(\"Cross-Ticker • Adjusted/Close Prices (Raw Levels)\")\n",
        "        ax.set_xlabel(\"Date\")\n",
        "        ax.set_ylabel(\"Price / Level\")\n",
        "        ax.legend(loc=\"best\")\n",
        "        figs.append(fig)\n",
        "\n",
        "    return figs\n",
        "\n",
        "# ----------------------------\n",
        "# Report Orchestration\n",
        "# ----------------------------\n",
        "def build_multi_asset_report(\n",
        "    frames: Dict[str, pd.DataFrame],\n",
        "    pdf_title: str = \"Multi-Asset EDA report.pdf\",\n",
        "    show_plots: bool = True\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Build a multi-page PDF report covering stylized facts & seasonality\n",
        "    for single-ticker frames.\n",
        "\n",
        "    Seasonality is derived from the Date index (DatetimeIndex).\n",
        "    \"\"\"\n",
        "    ordered_items = sorted(frames.items(), key=lambda kv: kv[0])\n",
        "    figs: List[plt.Figure] = []\n",
        "\n",
        "    # Per-ticker\n",
        "    for name, df in ordered_items:\n",
        "        figs.extend(eda_single_ticker(name, df))\n",
        "\n",
        "    # Cross-ticker overlay\n",
        "    figs.extend(eda_cross_ticker_adjclose(frames))\n",
        "\n",
        "    with PdfPages(pdf_title) as pdf:\n",
        "        # Cover page\n",
        "        cover_fig, ax = plt.subplots()\n",
        "        ax.axis(\"off\")\n",
        "        ax.text(0.0, 0.9, \"Multi-Asset EDA Report\", fontsize=22, fontweight=\"bold\",\n",
        "                ha=\"left\", va=\"top\")\n",
        "        tickers_list = \", \".join([_safe_title(k) for k, _ in ordered_items])\n",
        "        ax.text(0.0, 0.78, f\"Tickers: {tickers_list}\", fontsize=12, ha=\"left\", va=\"top\")\n",
        "        ax.text(0.0, 0.72, \"Contents:\", fontsize=14, fontweight=\"bold\", ha=\"left\", va=\"top\")\n",
        "        contents = [\n",
        "            \"• Price series & ACF(Price), Moving Averages (5/10/20)\",\n",
        "            \"• Daily log-returns time series; Histogram+KDE (with Std Dev); Q–Q plot\",\n",
        "            \"• ACF(Returns), Rolling Autocorr(Returns)\",\n",
        "            \"• Volatility clustering: ACF(|r|) & ACF(r^2)\",\n",
        "            \"• Seasonality (from Date index): Monthly/DOW/Quarterly boxplots; Month×Year heatmap; Month-End/Start bars\",\n",
        "            \"• Feature distributions (Spreads/MAs/Lags) & correlation heatmaps\",\n",
        "            \"• Cross-ticker adjusted/close overlay\",\n",
        "        ]\n",
        "        for i, line in enumerate(contents):\n",
        "            ax.text(0.02, 0.66 - i * 0.05, line, fontsize=12, ha=\"left\", va=\"top\")\n",
        "        pdf.savefig(cover_fig)\n",
        "        plt.close(cover_fig)\n",
        "\n",
        "        # Content pages\n",
        "        for fig in figs:\n",
        "            pdf.savefig(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "    print(f\"✅ PDF saved: {pdf_title}\")\n",
        "\n",
        "    if show_plots:\n",
        "        # Recreate for display (since figures were closed after saving)\n",
        "        display_figs: List[plt.Figure] = []\n",
        "        for name, df in ordered_items:\n",
        "            display_figs.extend(eda_single_ticker(name, df))\n",
        "        display_figs.extend(eda_cross_ticker_adjclose(frames))\n",
        "        for fig in display_figs:\n",
        "            plt.show()\n",
        "\n",
        "# ----------------------------\n",
        "# Backward-compatible alias (optional)\n",
        "# ----------------------------\n",
        "def build_energy_etf_report(frames: Dict[str, pd.DataFrame],\n",
        "                            pdf_title: str = \"EDA energy ETF report.pdf\",\n",
        "                            show_plots: bool = True) -> None:\n",
        "    \"\"\"Alias to keep older notebooks working.\"\"\"\n",
        "    build_multi_asset_report(frames, pdf_title=pdf_title, show_plots=show_plots)\n",
        "\n",
        "# ----------------------------\n",
        "# Example usage (your requested frames)\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # DataFrames are assumed to already exist from earlier cells/code:\n",
        "    # spy_df, gld_df, clf_df, psx_df, vlo_df, mpc_df, eurusd_df\n",
        "\n",
        "    frames = {\n",
        "        \"SPY\": spy_df,\n",
        "        \"GLD\": gld_df,\n",
        "        \"CL=F\": clf_df,\n",
        "        \"PSX\": psx_df,\n",
        "        \"VLO\": vlo_df,\n",
        "        \"MPC\": mpc_df,\n",
        "        \"EUR_USD\": eurusd_df,\n",
        "    }\n",
        "\n",
        "    build_multi_asset_report(frames, pdf_title=\"Multi-Asset Stylized Facts report.pdf\", show_plots=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_11276/866163125.py:401: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(labels, rotation=0)\n/tmp/ipykernel_11276/866163125.py:478: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=res, x=\"Group\", y=\"AvgRet\", palette=\"Blues_d\", ax=ax)\n/tmp/ipykernel_11276/866163125.py:194: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n  fig, ax = plt.subplots()\n/tmp/ipykernel_11276/866163125.py:401: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(labels, rotation=0)\n/tmp/ipykernel_11276/866163125.py:478: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=res, x=\"Group\", y=\"AvgRet\", palette=\"Blues_d\", ax=ax)\n/tmp/ipykernel_11276/866163125.py:401: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(labels, rotation=0)\n/tmp/ipykernel_11276/866163125.py:478: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=res, x=\"Group\", y=\"AvgRet\", palette=\"Blues_d\", ax=ax)\n/tmp/ipykernel_11276/866163125.py:401: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(labels, rotation=0)\n/tmp/ipykernel_11276/866163125.py:478: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=res, x=\"Group\", y=\"AvgRet\", palette=\"Blues_d\", ax=ax)\n/tmp/ipykernel_11276/866163125.py:401: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(labels, rotation=0)\n/tmp/ipykernel_11276/866163125.py:478: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=res, x=\"Group\", y=\"AvgRet\", palette=\"Blues_d\", ax=ax)\n/tmp/ipykernel_11276/866163125.py:401: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(labels, rotation=0)\n/tmp/ipykernel_11276/866163125.py:478: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=res, x=\"Group\", y=\"AvgRet\", palette=\"Blues_d\", ax=ax)\n/tmp/ipykernel_11276/866163125.py:401: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(labels, rotation=0)\n/tmp/ipykernel_11276/866163125.py:478: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=res, x=\"Group\", y=\"AvgRet\", palette=\"Blues_d\", ax=ax)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "✅ PDF saved: Multi-Asset Stylized Facts report.pdf\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1771455669074
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate GARCH Report"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Univariate GARCH(1,1) Volatility & Diagnostics Report (Multi-Asset)\n",
        "\n",
        "Updates included:\n",
        "  • h-step forecast horizon default set to 90 days.\n",
        "  • Volatility series are shown as ANNUALIZED PERCENT values so they can be compared.\n",
        "  • Adds 100-day realized volatility trend (annualized %) overlaid on conditional volatility plot.\n",
        "  • Removes lag-0 (day 0) from all ACF plots.\n",
        "  • ACF plots include an explicit explanation of the shaded region (significance band).\n",
        "  • ACF plots auto-scale the y-axis based on data (and CI band) for readability.\n",
        "  • Metrics table now includes a simple explanation for each metric.\n",
        "\n",
        "Inputs:\n",
        "    Assumes we already have engineered feature DataFrames in memory, keyed by asset name,\n",
        "    e.g.: spy_df, gld_df, clf_df, psx_df, vlo_df, mpc_df, eurusd_df\n",
        "\n",
        "Output (default):\n",
        "    \"Univariate_GARCH_Volatility_Report.pdf\"\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import textwrap  # <-- ADDED (for table cell wrapping)\n",
        "\n",
        "from typing import Dict, Optional, List, Tuple\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from scipy import stats\n",
        "\n",
        "# --- econometrics ---\n",
        "from arch import arch_model\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.tsa.stattools import acf as sm_acf\n",
        "\n",
        "# ----------------------------\n",
        "# Global settings\n",
        "# ----------------------------\n",
        "TRADING_DAYS_PER_YEAR = 252\n",
        "REALIZED_VOL_WINDOW_DAYS = 100\n",
        "DEFAULT_FORECAST_H = 90\n",
        "\n",
        "# ACF plotting defaults\n",
        "ACF_ALPHA = 0.05  # 95% significance band\n",
        "ACF_LAGS_DEFAULT = 20\n",
        "\n",
        "# ----------------------------\n",
        "# Plot styling\n",
        "# ----------------------------\n",
        "plt.rcParams[\"figure.figsize\"] = (11, 7)\n",
        "plt.rcParams[\"axes.grid\"] = True\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def _safe_title(name: str) -> str:\n",
        "    return str(name).strip() if name else \"Unknown\"\n",
        "\n",
        "\n",
        "def _numeric_clean(series: pd.Series) -> pd.Series:\n",
        "    return pd.to_numeric(series, errors=\"coerce\").astype(float)\n",
        "\n",
        "\n",
        "def _infer_engineered_logret_col(df: pd.DataFrame, asset_name: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Prefer exact engineered naming: log_ret|{asset_name}\n",
        "    Fallback: any column that starts with 'log_ret|' (first match).\n",
        "    \"\"\"\n",
        "    exact = f\"log_ret|{asset_name}\"\n",
        "    if exact in df.columns:\n",
        "        return exact\n",
        "    for c in df.columns:\n",
        "        if str(c).lower().startswith(\"log_ret|\"):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "\n",
        "def _infer_price_col(df: pd.DataFrame, asset_name: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Prefer: Adj Close|{asset}, else Close|{asset}\n",
        "    Fallback: any column containing 'adj close' else any containing 'close'\n",
        "    \"\"\"\n",
        "    direct = [f\"Adj Close|{asset_name}\", f\"Close|{asset_name}\"]\n",
        "    for c in direct:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "\n",
        "    for c in df.columns:\n",
        "        if \"adj close\" in str(c).lower():\n",
        "            return c\n",
        "    for c in df.columns:\n",
        "        if \"close\" in str(c).lower():\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "\n",
        "def _get_returns(df: pd.DataFrame, asset_name: str) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Get engineered log returns if present; else compute from inferred Adj/Close.\n",
        "    Returns a pandas Series with a DatetimeIndex if possible.\n",
        "    \"\"\"\n",
        "    col = _infer_engineered_logret_col(df, asset_name)\n",
        "    if col is not None:\n",
        "        r = _numeric_clean(df[col]).dropna().sort_index()\n",
        "        if r.size > 0:\n",
        "            return r\n",
        "\n",
        "    px_col = _infer_price_col(df, asset_name)\n",
        "    if px_col is None:\n",
        "        return pd.Series(dtype=float)\n",
        "\n",
        "    p = _numeric_clean(df[px_col]).sort_index()\n",
        "    p = p.replace([0, np.inf, -np.inf], np.nan)\n",
        "    return np.log(p / p.shift(1)).dropna()\n",
        "\n",
        "\n",
        "def _qqplot_fig(series: pd.Series, title: str) -> plt.Figure:\n",
        "    fig, ax = plt.subplots()\n",
        "    stats.probplot(series.dropna(), dist=\"norm\", plot=ax)\n",
        "    ax.set_title(title)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def _table_figure_from_df(title: str, df: pd.DataFrame) -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Render a DataFrame as a nicely formatted matplotlib table.\n",
        "\n",
        "    UPDATED:\n",
        "      - Wraps text within cells so long descriptions don't overflow.\n",
        "      - Adjusts column widths and row heights to accommodate wrapped text.\n",
        "    \"\"\"\n",
        "    # dynamic height\n",
        "    fig_h = min(9, 1.2 + 0.38 * (len(df) + 2))\n",
        "    fig, ax = plt.subplots(figsize=(11, fig_h))\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(title, fontsize=14, loc=\"left\")\n",
        "\n",
        "    # ---- Wrap text content per-column (character-based) ----\n",
        "    # You can tweak these widths if you want different wrapping behavior.\n",
        "    wrap_width_by_col = {\n",
        "        0: 24,   # Metric\n",
        "        1: 18,   # Value\n",
        "        2: 70,   # Explanation (long)\n",
        "    }\n",
        "\n",
        "    def _wrap_cell(val: object, col_idx: int) -> str:\n",
        "        s = \"\" if val is None else str(val)\n",
        "        s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "        width = wrap_width_by_col.get(col_idx, 30)\n",
        "        # Preserve explicit newlines while wrapping each line.\n",
        "        lines = []\n",
        "        for line in s.split(\"\\n\"):\n",
        "            if line.strip() == \"\":\n",
        "                lines.append(\"\")\n",
        "            else:\n",
        "                lines.append(textwrap.fill(line, width=width, break_long_words=False))\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    cell_text = []\n",
        "    for row in df.values.tolist():\n",
        "        wrapped_row = []\n",
        "        for j, v in enumerate(row):\n",
        "            wrapped_row.append(_wrap_cell(v, j))\n",
        "        cell_text.append(wrapped_row)\n",
        "\n",
        "    col_labels = [str(c) for c in df.columns]\n",
        "\n",
        "    tbl = ax.table(\n",
        "        cellText=cell_text,\n",
        "        colLabels=col_labels,\n",
        "        loc=\"center\",\n",
        "        cellLoc=\"left\",\n",
        "    )\n",
        "\n",
        "    # Font and scale\n",
        "    tbl.auto_set_font_size(False)\n",
        "    tbl.set_fontsize(9.5)\n",
        "    tbl.scale(1, 1.18)\n",
        "\n",
        "    # ---- Set column widths (fractions of axes width) ----\n",
        "    # If there are only 2 columns (Metric/Value), keep it balanced.\n",
        "    ncols = len(df.columns)\n",
        "    if ncols == 3:\n",
        "        col_widths = {0: 0.23, 1: 0.17, 2: 0.60}\n",
        "    elif ncols == 2:\n",
        "        col_widths = {0: 0.38, 1: 0.62}\n",
        "    else:\n",
        "        # fallback: equal widths\n",
        "        col_widths = {j: 1.0 / max(ncols, 1) for j in range(ncols)}\n",
        "\n",
        "    # Apply widths and enable text wrapping in every cell\n",
        "    for (r, c), cell in tbl.get_celld().items():\n",
        "        if c in col_widths:\n",
        "            cell.set_width(col_widths[c])\n",
        "        cell.get_text().set_wrap(True)\n",
        "\n",
        "        # Header styling (row 0 in matplotlib table is header)\n",
        "        if r == 0:\n",
        "            cell.set_facecolor(\"#F2F2F2\")\n",
        "            cell.get_text().set_weight(\"bold\")\n",
        "\n",
        "    # ---- Adjust row heights based on wrapped line count ----\n",
        "    # Determine max number of lines per row (including header)\n",
        "    base_h = tbl[(1, 0)].get_height() if (1, 0) in tbl.get_celld() else 0.05\n",
        "    for r in range(1, len(df) + 1):  # data rows start at 1\n",
        "        max_lines = 1\n",
        "        for c in range(ncols):\n",
        "            txt = tbl[(r, c)].get_text().get_text()\n",
        "            max_lines = max(max_lines, txt.count(\"\\n\") + 1)\n",
        "        # Increase height proportionally (tuned factor)\n",
        "        new_h = base_h * (0.95 + 0.55 * (max_lines - 1))\n",
        "        for c in range(ncols):\n",
        "            tbl[(r, c)].set_height(new_h)\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def _table_figure_from_dict(title: str, rows: Dict[str, object]) -> plt.Figure:\n",
        "    df = pd.DataFrame({\"Metric\": list(rows.keys()),\n",
        "                       \"Value\": [rows[k] for k in rows]})\n",
        "    return _table_figure_from_df(title, df)\n",
        "\n",
        "\n",
        "def _half_life(alpha: float, beta: float) -> Optional[float]:\n",
        "    \"\"\"Half-life in days: ln(0.5) / ln(alpha+beta), if 0 < alpha+beta < 1.\"\"\"\n",
        "    rho = alpha + beta\n",
        "    if rho <= 0 or rho >= 1:\n",
        "        return None\n",
        "    return math.log(0.5) / math.log(rho)\n",
        "\n",
        "\n",
        "def _annualize_vol_to_pct(daily_vol: pd.Series, scale: int = TRADING_DAYS_PER_YEAR) -> pd.Series:\n",
        "    \"\"\"Convert daily volatility (std dev of daily returns) to annualized %.\"\"\"\n",
        "    return daily_vol * np.sqrt(scale) * 100.0\n",
        "\n",
        "\n",
        "def _annualize_var_to_pct2(daily_var: pd.Series, scale: int = TRADING_DAYS_PER_YEAR) -> pd.Series:\n",
        "    \"\"\"Convert daily variance to annualized percent^2 (variance units).\"\"\"\n",
        "    return daily_var * scale * (100.0 ** 2)\n",
        "\n",
        "\n",
        "def _format_number(x, ndigits: int = 6) -> str:\n",
        "    if x is None:\n",
        "        return \"—\"\n",
        "    try:\n",
        "        if isinstance(x, (float, np.floating, int, np.integer)) and not np.isnan(float(x)):\n",
        "            # use scientific for very small or very large\n",
        "            ax = abs(float(x))\n",
        "            if ax != 0 and (ax < 1e-4 or ax > 1e4):\n",
        "                return f\"{float(x):.{ndigits}e}\"\n",
        "            return f\"{float(x):.{ndigits}f}\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    return str(x)\n",
        "\n",
        "\n",
        "def _metrics_table_with_explanations(title: str, metrics: Dict[str, object], dist: str) -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Build a metrics table with a plain-language explanation column.\n",
        "    \"\"\"\n",
        "    explanations = {\n",
        "        \"μ (mean)\": \"Estimated average daily return (constant mean in the return equation).\",\n",
        "        \"ω\": \"Variance intercept; baseline level feeding the long-run variance.\",\n",
        "        \"α\": \"Shock (ARCH) effect; how strongly yesterday’s squared residual increases today’s variance.\",\n",
        "        \"β\": \"Persistence (GARCH) effect; how strongly yesterday’s variance carries into today.\",\n",
        "        \"ν (Student‑t df)\": \"Student‑t degrees of freedom (lower ν = heavier tails). Only relevant for Student‑t innovations.\",\n",
        "        \"α+β (persistence)\": \"Total variance persistence; closer to 1 implies slower mean reversion and stronger volatility clustering.\",\n",
        "        \"σ∞² = ω/(1−α−β)\": \"Long-run (unconditional) variance implied by the model, assuming α+β < 1.\",\n",
        "        \"Half‑life (days)\": \"Approx. days for a volatility shock to decay by 50% (based on α+β).\",\n",
        "        \"log‑likelihood\": \"Model fit objective value under maximum likelihood; higher is better (within same data/model).\",\n",
        "        \"AIC\": \"Akaike Information Criterion (penalized fit); lower is better for comparing models on the same data.\",\n",
        "        \"BIC\": \"Bayesian Information Criterion (stronger penalty than AIC); lower is better for comparing models on the same data.\"\n",
        "    }\n",
        "\n",
        "    rows = []\n",
        "    for k, v in metrics.items():\n",
        "        # keep Student-t row meaningful\n",
        "        if k == \"ν (Student‑t df)\" and dist != \"t\":\n",
        "            expl = \"Not estimated under Gaussian innovations.\"\n",
        "        else:\n",
        "            expl = explanations.get(k, \"\")\n",
        "        rows.append([k, _format_number(v, ndigits=6), expl])\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\"Metric\", \"Value\", \"Explanation\"])\n",
        "    return _table_figure_from_df(title, df)\n",
        "\n",
        "# ----------------------------\n",
        "# ACF plotting with shaded-band explanation + autoscaling + lag0 omitted\n",
        "# ----------------------------\n",
        "def _annotate_acf_band(ax: plt.Axes, alpha: float) -> None:\n",
        "    \"\"\"\n",
        "    Add an on-plot explanation for the shaded confidence band.\n",
        "    \"\"\"\n",
        "    pct = int(round((1.0 - alpha) * 100))\n",
        "    note = (\n",
        "        f\"Shaded band: ±{pct}% significance bounds around 0 under H₀ (no autocorrelation).\\n\"\n",
        "        f\"Bars outside the band are statistically significant at α={alpha:.2f}.\"\n",
        "    )\n",
        "    ax.text(\n",
        "        0.02, 0.02, note,\n",
        "        transform=ax.transAxes,\n",
        "        va=\"bottom\", ha=\"left\",\n",
        "        fontsize=9,\n",
        "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.85, edgecolor=\"lightgray\")\n",
        "    )\n",
        "\n",
        "\n",
        "def _set_acf_ylim(ax: plt.Axes, y_values: np.ndarray, ci_low: Optional[np.ndarray] = None, ci_high: Optional[np.ndarray] = None) -> None:\n",
        "    \"\"\"\n",
        "    Auto-scale y-axis based on data and (optional) CI band, with some padding.\n",
        "    \"\"\"\n",
        "    arrs = [np.asarray(y_values)]\n",
        "    if ci_low is not None:\n",
        "        arrs.append(np.asarray(ci_low))\n",
        "    if ci_high is not None:\n",
        "        arrs.append(np.asarray(ci_high))\n",
        "    mx = float(np.nanmax(np.abs(np.concatenate([a[~np.isnan(a)] for a in arrs if a.size > 0])))) if arrs else 1.0\n",
        "    mx = max(mx, 0.05)  # prevent too-tight scale when correlations are tiny\n",
        "    pad = 1.25 * mx\n",
        "    ax.set_ylim(-pad, pad)\n",
        "\n",
        "\n",
        "def _plot_acf_omit_lag0(x: pd.Series, lags: int, title: str, alpha: float = ACF_ALPHA) -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Plot ACF excluding lag 0 (day 0), with:\n",
        "      - shaded confidence band explanation\n",
        "      - auto y-axis scaling\n",
        "      - consistent behavior across statsmodels versions\n",
        "\n",
        "    Strategy:\n",
        "      1) Try statsmodels.graphics.tsaplots.plot_acf with zero=False, alpha=alpha, auto_ylims=True.\n",
        "      2) If options are unsupported (older statsmodels), manually compute acf + confint using\n",
        "         statsmodels.tsa.stattools.acf and plot with band centered at 0.\n",
        "    \"\"\"\n",
        "    x_clean = pd.Series(x).dropna().astype(float)\n",
        "\n",
        "    # --- attempt using plot_acf (best-looking, includes band) ---\n",
        "    try:\n",
        "        fig = plot_acf(\n",
        "            x_clean,\n",
        "            lags=lags,\n",
        "            zero=False,           # omit lag 0\n",
        "            alpha=alpha,          # show band\n",
        "            auto_ylims=True       # auto scale y based on values & band (if supported)\n",
        "        )\n",
        "        ax = fig.axes[0]\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel(\"Lag\")\n",
        "        ax.set_ylabel(\"Autocorrelation\")\n",
        "        _annotate_acf_band(ax, alpha=alpha)\n",
        "        return fig\n",
        "    except TypeError:\n",
        "        # --- manual fallback: compute acf + confint and plot with shaded band ---\n",
        "        acf_vals, confint = sm_acf(x_clean.values, nlags=lags, alpha=alpha, fft=True)\n",
        "\n",
        "        # omit lag 0\n",
        "        lag_idx = np.arange(1, lags + 1)\n",
        "        vals = acf_vals[1:lags + 1]\n",
        "        ci = confint[1:lags + 1, :]  # [low, high] for each lag\n",
        "\n",
        "        # statsmodels plot_acf shades confint centered at 0: (confint - acf_vals)\n",
        "        band_low = ci[:, 0] - vals\n",
        "        band_high = ci[:, 1] - vals\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(11, 6))\n",
        "        ax.axhline(0.0, color=\"gray\", lw=0.8)\n",
        "\n",
        "        # shade CI band\n",
        "        ax.fill_between(\n",
        "            lag_idx.astype(float),\n",
        "            band_low,\n",
        "            band_high,\n",
        "            color=\"C0\",\n",
        "            alpha=0.20,\n",
        "            edgecolor=\"none\"\n",
        "        )\n",
        "\n",
        "        # stem-like plot\n",
        "        ax.vlines(lag_idx, 0.0, vals, colors=\"C0\", lw=1.2)\n",
        "        ax.plot(lag_idx, vals, \"o\", color=\"C0\", markersize=4)\n",
        "\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel(\"Lag\")\n",
        "        ax.set_ylabel(\"Autocorrelation\")\n",
        "\n",
        "        _set_acf_ylim(ax, vals, band_low, band_high)\n",
        "        ax.set_xlim(0.5, lags + 0.5)\n",
        "        _annotate_acf_band(ax, alpha=alpha)\n",
        "\n",
        "        return fig\n",
        "\n",
        "# ----------------------------\n",
        "# GARCH(1,1) estimation per asset\n",
        "# ----------------------------\n",
        "def fit_garch11(\n",
        "    returns: pd.Series,\n",
        "    dist: str = \"normal\",          # \"normal\" or \"t\"\n",
        "    robust_se: bool = True\n",
        ") -> Tuple[object, Dict[str, object], pd.Index]:\n",
        "    \"\"\"\n",
        "    Estimate GARCH(1,1) with mean=constant (μ), variance=ω+α e_{t-1}^2 + β σ_{t-1}^2.\n",
        "    Returns fitted result, metrics dict, and the fit index for plotting.\n",
        "    \"\"\"\n",
        "    r = returns.dropna().astype(float)\n",
        "    if r.empty:\n",
        "        raise ValueError(\"Empty returns series provided to fit_garch11().\")\n",
        "\n",
        "    fit_index = r.index\n",
        "\n",
        "    am = arch_model(\n",
        "        r,\n",
        "        mean=\"constant\",\n",
        "        vol=\"GARCH\",\n",
        "        p=1, q=1,\n",
        "        dist=\"normal\" if dist == \"normal\" else \"t\",\n",
        "        rescale=False,\n",
        "    )\n",
        "\n",
        "    fit_kwargs = dict(disp=\"off\", update_freq=0, show_warning=False)\n",
        "    if robust_se:\n",
        "        fit_kwargs[\"cov_type\"] = \"robust\"\n",
        "\n",
        "    res = am.fit(**fit_kwargs)\n",
        "\n",
        "    params = res.params\n",
        "    mu = float(params.get(\"mu\", np.nan))\n",
        "    omega = float(params.get(\"omega\", np.nan))\n",
        "    alpha = float(params.get(\"alpha[1]\", np.nan))\n",
        "    beta  = float(params.get(\"beta[1]\", np.nan))\n",
        "    nu    = float(params.get(\"nu\", np.nan)) if dist == \"t\" and \"nu\" in params.index else np.nan\n",
        "\n",
        "    rho = alpha + beta\n",
        "    sig2_inf = np.nan\n",
        "    if (1 - rho) > 1e-12:\n",
        "        sig2_inf = omega / (1 - rho)\n",
        "\n",
        "    hl = _half_life(alpha, beta)\n",
        "\n",
        "    metrics = {\n",
        "        \"μ (mean)\": mu,\n",
        "        \"ω\": omega,\n",
        "        \"α\": alpha,\n",
        "        \"β\": beta,\n",
        "        \"ν (Student‑t df)\": nu if dist == \"t\" else \"—\",\n",
        "        \"α+β (persistence)\": rho,\n",
        "        \"σ∞² = ω/(1−α−β)\": sig2_inf,\n",
        "        \"Half‑life (days)\": hl,\n",
        "        \"log‑likelihood\": float(res.loglikelihood),\n",
        "        \"AIC\": float(res.aic),\n",
        "        \"BIC\": float(res.bic),\n",
        "    }\n",
        "    return res, metrics, fit_index\n",
        "\n",
        "# ----------------------------\n",
        "# Plots\n",
        "# ----------------------------\n",
        "def _plot_volatility_path(\n",
        "    res,\n",
        "    fit_index: pd.Index,\n",
        "    returns: pd.Series,\n",
        "    title_prefix: str,\n",
        "    realized_window: int = REALIZED_VOL_WINDOW_DAYS\n",
        ") -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Plot conditional variance (annualized %^2) and volatility (annualized %),\n",
        "    and overlay realized volatility (rolling window, annualized %).\n",
        "    \"\"\"\n",
        "    sig_daily = pd.Series(np.asarray(res.conditional_volatility), index=fit_index).dropna()\n",
        "\n",
        "    sig_ann_pct = _annualize_vol_to_pct(sig_daily)\n",
        "    sig2_ann_pct2 = _annualize_var_to_pct2(sig_daily ** 2)\n",
        "\n",
        "    r_aligned = pd.Series(returns).reindex(fit_index).astype(float)\n",
        "    realized_ann_pct = _annualize_vol_to_pct(\n",
        "        r_aligned.rolling(realized_window).std(ddof=1)\n",
        "    )\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 8), sharex=True)\n",
        "\n",
        "    ax1.plot(sig2_ann_pct2.index, sig2_ann_pct2.values, color=\"firebrick\", lw=1.2)\n",
        "    ax1.set_title(f\"{title_prefix} • Conditional Variance (Annualized, %²)\")\n",
        "    ax1.set_ylabel(\"Annualized Variance (%²)\")\n",
        "\n",
        "    ax2.plot(sig_ann_pct.index, sig_ann_pct.values, color=\"royalblue\", lw=1.2,\n",
        "             label=\"Conditional Vol (GARCH, ann. %)\")\n",
        "    ax2.plot(realized_ann_pct.index, realized_ann_pct.values, color=\"darkorange\", lw=1.1, ls=\"--\",\n",
        "             label=f\"Realized Vol ({realized_window}D, ann. %)\")\n",
        "\n",
        "    ax2.set_title(f\"{title_prefix} • Volatility (Annualized %) • Conditional vs Realized\")\n",
        "    ax2.set_xlabel(\"Date\")\n",
        "    ax2.set_ylabel(\"Annualized Volatility (%)\")\n",
        "    ax2.legend(loc=\"best\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# --- ADDED: plot daily percent returns together with conditional volatility ---\n",
        "def _plot_daily_pct_returns_with_conditional_vol(\n",
        "    res,\n",
        "    fit_index: pd.Index,\n",
        "    returns: pd.Series,\n",
        "    title_prefix: str\n",
        ") -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Plot DAILY percent returns together with CONDITIONAL volatility.\n",
        "\n",
        "    - Left axis: daily returns in percent (100 * log return)\n",
        "    - Right axis: conditional volatility annualized percent (daily sigma * sqrt(252) * 100)\n",
        "\n",
        "    Uses a twin y-axis so both series are readable on their natural scales.\n",
        "    \"\"\"\n",
        "    r_aligned = pd.Series(returns).reindex(fit_index).astype(float)\n",
        "    r_pct = 100.0 * r_aligned\n",
        "\n",
        "    sig_daily = pd.Series(np.asarray(res.conditional_volatility), index=fit_index).dropna()\n",
        "    sig_ann_pct = _annualize_vol_to_pct(sig_daily)\n",
        "\n",
        "    # Align on common index to avoid plotting gaps/misalignment\n",
        "    common_idx = r_pct.index.intersection(sig_ann_pct.index)\n",
        "    r_pct = r_pct.reindex(common_idx)\n",
        "    sig_ann_pct = sig_ann_pct.reindex(common_idx)\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(11, 7))\n",
        "\n",
        "    ax1.plot(r_pct.index, r_pct.values, color=\"slategray\", lw=0.9, alpha=0.75, label=\"Daily Return (%)\")\n",
        "    ax1.axhline(0.0, color=\"gray\", lw=0.8)\n",
        "    ax1.set_ylabel(\"Daily Return (%)\", color=\"slategray\")\n",
        "    ax1.tick_params(axis=\"y\", labelcolor=\"slategray\")\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(sig_ann_pct.index, sig_ann_pct.values, color=\"royalblue\", lw=1.2, label=\"Conditional Vol (ann. %)\")\n",
        "    ax2.set_ylabel(\"Conditional Volatility (Annualized %)\", color=\"royalblue\")\n",
        "    ax2.tick_params(axis=\"y\", labelcolor=\"royalblue\")\n",
        "\n",
        "    ax1.set_title(f\"{title_prefix} • Daily % Returns vs Conditional Volatility (Annualized %)\")\n",
        "    ax1.set_xlabel(\"Date\")\n",
        "\n",
        "    # Combined legend\n",
        "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    return fig\n",
        "# --- END ADDED ---\n",
        "\n",
        "\n",
        "def _plot_std_residuals(res, fit_index: pd.Index, title_prefix: str) -> plt.Figure:\n",
        "    z = pd.Series(np.asarray(res.std_resid), index=fit_index).dropna()\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(z.index, z.values, lw=1.0, color=\"slategray\")\n",
        "    ax.axhline(0, color=\"gray\", lw=0.8)\n",
        "    ax.set_title(f\"{title_prefix} • Standardized Residuals\")\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    return fig\n",
        "\n",
        "\n",
        "def _plot_acf_resid_sq(res, fit_index: pd.Index, lags: int, title_prefix: str) -> plt.Figure:\n",
        "    z = pd.Series(np.asarray(res.std_resid), index=fit_index).dropna()\n",
        "    z2 = z ** 2\n",
        "    return _plot_acf_omit_lag0(\n",
        "        z2,\n",
        "        lags=lags,\n",
        "        title=f\"{title_prefix} • ACF(Standardized Residuals²) [Lag 0 Omitted]\",\n",
        "        alpha=ACF_ALPHA\n",
        "    )\n",
        "\n",
        "\n",
        "def _variance_forecast_path(res, fit_index: pd.Index, h: int, title_prefix: str) -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Closed-form h-step VARIANCE forecast for GARCH(1,1):\n",
        "      σ_{T+k|T}^2 = σ∞² + (α+β)^k (σ_T^2 − σ∞²)\n",
        "\n",
        "    Plotted as implied h-step VOLATILITY forecast (annualized %).\n",
        "    \"\"\"\n",
        "    params = res.params\n",
        "    omega = float(params.get(\"omega\", np.nan))\n",
        "    alpha = float(params.get(\"alpha[1]\", np.nan))\n",
        "    beta  = float(params.get(\"beta[1]\", np.nan))\n",
        "    rho   = alpha + beta\n",
        "\n",
        "    sig_daily = pd.Series(np.asarray(res.conditional_volatility), index=fit_index).dropna()\n",
        "    last_sig2 = float(sig_daily.iloc[-1] ** 2)\n",
        "\n",
        "    if (1 - rho) <= 1e-12 or np.isnan(omega) or np.isnan(rho):\n",
        "        f_var_path = np.full(h, last_sig2)\n",
        "    else:\n",
        "        sig2_inf = omega / (1 - rho)\n",
        "        f_var_path = np.array([\n",
        "            sig2_inf + (rho ** k) * (last_sig2 - sig2_inf) for k in range(1, h + 1)\n",
        "        ])\n",
        "\n",
        "    f_vol_ann_pct = np.sqrt(np.maximum(f_var_path, 0.0)) * np.sqrt(TRADING_DAYS_PER_YEAR) * 100.0\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(range(1, h + 1), f_vol_ann_pct, marker=\"o\", lw=1.2, color=\"darkorange\")\n",
        "    ax.set_title(f\"{title_prefix} • h‑Step Volatility Forecast (Annualized %, Closed‑Form)\")\n",
        "    ax.set_xlabel(\"h (days ahead)\")\n",
        "    ax.set_ylabel(\"Annualized Volatility (%)\")\n",
        "    return fig\n",
        "\n",
        "\n",
        "def _stylized_facts_plots(r: pd.Series, title_prefix: str) -> List[plt.Figure]:\n",
        "    figs: List[plt.Figure] = []\n",
        "    r_clean = r.dropna().astype(float)\n",
        "\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    sns.histplot(r_clean, bins=60, kde=True, stat=\"density\", color=\"teal\", ax=ax1)\n",
        "    ax1.set_title(f\"{title_prefix} • Return Distribution\")\n",
        "    ax1.set_xlabel(\"Daily log return\")\n",
        "\n",
        "    mean, std = float(r_clean.mean()), float(r_clean.std(ddof=1))\n",
        "    skew = stats.skew(r_clean, bias=False)\n",
        "    kurt = stats.kurtosis(r_clean, fisher=True, bias=False)\n",
        "\n",
        "    box = \"\\n\".join([\n",
        "        f\"Mean: {mean:.6f}\",\n",
        "        f\"Std:  {std:.6f}\",\n",
        "        f\"Skew: {skew:.3f}\",\n",
        "        f\"Kurt: {kurt:.3f}\",\n",
        "    ])\n",
        "    ax1.text(\n",
        "        0.02, 0.98, box, transform=ax1.transAxes, va=\"top\", ha=\"left\",\n",
        "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.85)\n",
        "    )\n",
        "    ax1.axvline(mean, color=\"black\", ls=\"--\", lw=1.2, label=\"Mean\")\n",
        "    ax1.axvline(mean - std, color=\"gray\", ls=\"--\", lw=1.0)\n",
        "    ax1.axvline(mean + std, color=\"gray\", ls=\"--\", lw=1.0)\n",
        "    ax1.legend(loc=\"best\")\n",
        "    figs.append(fig1)\n",
        "\n",
        "    figs.append(_qqplot_fig(r_clean, f\"{title_prefix} • Q–Q Plot (Normal)\"))\n",
        "\n",
        "    figs.append(_plot_acf_omit_lag0(r_clean.abs(), lags=ACF_LAGS_DEFAULT,\n",
        "                                   title=f\"{title_prefix} • ACF(|r|) [Lag 0 Omitted]\", alpha=ACF_ALPHA))\n",
        "    figs.append(_plot_acf_omit_lag0((r_clean ** 2), lags=ACF_LAGS_DEFAULT,\n",
        "                                   title=f\"{title_prefix} • ACF(r²) [Lag 0 Omitted]\", alpha=ACF_ALPHA))\n",
        "\n",
        "    return figs\n",
        "\n",
        "# ----------------------------\n",
        "# Per-asset workflow\n",
        "# ----------------------------\n",
        "def garch_diagnostics_for_asset(\n",
        "    asset_name: str,\n",
        "    df: pd.DataFrame,\n",
        "    dist: str = \"normal\",\n",
        "    forecast_h: int = DEFAULT_FORECAST_H\n",
        ") -> List[plt.Figure]:\n",
        "    figs: List[plt.Figure] = []\n",
        "    title_prefix = _safe_title(asset_name)\n",
        "\n",
        "    r = _get_returns(df, asset_name)\n",
        "    if r.empty:\n",
        "        figs.append(_table_figure_from_dict(\n",
        "            f\"{title_prefix} • Data Warning\",\n",
        "            {\"Message\": \"No returns found or computed for this asset.\"}\n",
        "        ))\n",
        "        return figs\n",
        "\n",
        "    res, metrics, fit_index = fit_garch11(r, dist=dist, robust_se=True)\n",
        "\n",
        "    # Table with metric explanations\n",
        "    figs.append(_metrics_table_with_explanations(f\"{title_prefix} • GARCH(1,1) Estimates\", metrics, dist=dist))\n",
        "\n",
        "    figs.extend(_stylized_facts_plots(r, title_prefix))\n",
        "\n",
        "    # --- ADDED: daily percent returns plotted together with conditional volatility ---\n",
        "    figs.append(_plot_daily_pct_returns_with_conditional_vol(res, fit_index, returns=r, title_prefix=title_prefix))\n",
        "\n",
        "    figs.append(_plot_volatility_path(res, fit_index, returns=r, title_prefix=title_prefix,\n",
        "                                      realized_window=REALIZED_VOL_WINDOW_DAYS))\n",
        "    figs.append(_plot_std_residuals(res, fit_index, title_prefix))\n",
        "    figs.append(_plot_acf_resid_sq(res, fit_index, ACF_LAGS_DEFAULT, title_prefix))\n",
        "    figs.append(_variance_forecast_path(res, fit_index, forecast_h, title_prefix))\n",
        "\n",
        "    return figs\n",
        "\n",
        "# ----------------------------\n",
        "# Cross-asset appendix\n",
        "# ----------------------------\n",
        "def appendix_cross_asset(frames: Dict[str, pd.DataFrame]) -> List[plt.Figure]:\n",
        "    figs: List[plt.Figure] = []\n",
        "\n",
        "    ret_map = {}\n",
        "    for name, df in frames.items():\n",
        "        r = _get_returns(df, name)\n",
        "        if not r.empty:\n",
        "            ret_map[name] = r\n",
        "\n",
        "    if ret_map:\n",
        "        rets = pd.DataFrame(ret_map).sort_index()\n",
        "        fig, ax = plt.subplots()\n",
        "        sns.heatmap(rets.corr(), annot=True, cmap=\"viridis\", ax=ax)\n",
        "        ax.set_title(\"Appendix • Cross‑Asset Daily Log Return Correlation\")\n",
        "        figs.append(fig)\n",
        "\n",
        "    return figs\n",
        "\n",
        "# ----------------------------\n",
        "# PDF orchestration\n",
        "# ----------------------------\n",
        "def build_garch_volatility_report(\n",
        "    frames: Dict[str, pd.DataFrame],\n",
        "    pdf_title: str = \"Univariate_GARCH_Volatility_Report.pdf\",\n",
        "    use_student_t: bool = False,\n",
        "    forecast_h: int = DEFAULT_FORECAST_H,\n",
        "    show_plots: bool = False\n",
        ") -> None:\n",
        "    dist = \"t\" if use_student_t else \"normal\"\n",
        "    ordered = sorted(frames.items(), key=lambda kv: kv[0])\n",
        "\n",
        "    figs: List[plt.Figure] = []\n",
        "\n",
        "    settings = {\n",
        "        \"Model\": \"GARCH(1,1) with mean=constant; ML estimation\",\n",
        "        \"Innovation Distribution\": \"Student‑t\" if use_student_t else \"Gaussian\",\n",
        "        \"Forecast Horizon (h)\": forecast_h,\n",
        "        \"Volatility Units\": \"Annualized % (daily σ * √252 * 100)\",\n",
        "        \"Realized Vol Window\": f\"{REALIZED_VOL_WINDOW_DAYS} trading days (annualized %)\",\n",
        "        \"ACF Shaded Band\": f\"{int((1-ACF_ALPHA)*100)}% bounds around 0 under H₀ (no autocorrelation)\",\n",
        "        \"Assets Included\": \", \".join([k for k, _ in ordered]),\n",
        "    }\n",
        "    figs.append(_table_figure_from_dict(\"Report Settings\", settings))\n",
        "\n",
        "    for name, df in ordered:\n",
        "        if df is None or df.empty:\n",
        "            figs.append(_table_figure_from_dict(\n",
        "                f\"{_safe_title(name)} • Data Warning\",\n",
        "                {\"Message\": \"Empty DataFrame provided for this asset.\"}\n",
        "            ))\n",
        "            continue\n",
        "        figs.extend(garch_diagnostics_for_asset(name, df, dist=dist, forecast_h=forecast_h))\n",
        "\n",
        "    figs.extend(appendix_cross_asset({k: v for k, v in ordered if v is not None and not v.empty}))\n",
        "\n",
        "    with PdfPages(pdf_title) as pdf:\n",
        "        cover, ax = plt.subplots()\n",
        "        ax.axis(\"off\")\n",
        "        ax.text(0.02, 0.90, \"Univariate GARCH Volatility & Diagnostics Report\",\n",
        "                fontsize=20, weight=\"bold\")\n",
        "        ax.text(0.02, 0.82, \"Assets: \" + \", \".join([k for k, _ in ordered]), fontsize=12)\n",
        "        ax.text(0.02, 0.76,\n",
        "                \"This report estimates a GARCH(1,1) per asset, visualizes annualized conditional volatility, \"\n",
        "                \"overlays realized volatility (100D), checks residual diagnostics (including ACF with significance bands), \"\n",
        "                \"and produces closed‑form h‑step forecasts (shown as annualized volatility).\",\n",
        "                fontsize=11)\n",
        "        pdf.savefig(cover)\n",
        "        plt.close(cover)\n",
        "\n",
        "        for fig in figs:\n",
        "            pdf.savefig(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "    print(f\"✅ PDF saved: {pdf_title}\")\n",
        "\n",
        "    if show_plots:\n",
        "        for fig in figs:\n",
        "            fig.show()\n",
        "\n",
        "# ----------------------------\n",
        "# Example usage\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    frames = {\n",
        "        \"SPY\": spy_df,\n",
        "        \"GLD\": gld_df,\n",
        "        \"CL=F\": clf_df,\n",
        "        \"PSX\": psx_df,\n",
        "        \"VLO\": vlo_df,\n",
        "        \"MPC\": mpc_df,\n",
        "        \"EUR_USD\": eurusd_df,\n",
        "    }\n",
        "\n",
        "    build_garch_volatility_report(\n",
        "        frames,\n",
        "        pdf_title=\"Univariate_GARCH_Volatility_Report.pdf\",\n",
        "        use_student_t=False,\n",
        "        forecast_h=90,\n",
        "        show_plots=False\n",
        "    )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_11276/3182991058.py:601: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n  fig, ax = plt.subplots()\n/tmp/ipykernel_11276/3182991058.py:754: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n  pdf.savefig(cover)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "✅ PDF saved: Univariate_GARCH_Volatility_Report.pdf\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1771455760698
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}